{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":658267,"sourceType":"datasetVersion","datasetId":277323},{"sourceId":10913508,"sourceType":"datasetVersion","datasetId":6782573},{"sourceId":13086054,"sourceType":"datasetVersion","datasetId":8288443},{"sourceId":13428164,"sourceType":"datasetVersion","datasetId":8522886},{"sourceId":13502735,"sourceType":"datasetVersion","datasetId":8573067},{"sourceId":602306,"sourceType":"modelInstanceVersion","modelInstanceId":451447,"modelId":467769},{"sourceId":603319,"sourceType":"modelInstanceVersion","modelInstanceId":452294,"modelId":468612},{"sourceId":612131,"sourceType":"modelInstanceVersion","modelInstanceId":459835,"modelId":475679},{"sourceId":612211,"sourceType":"modelInstanceVersion","modelInstanceId":459901,"modelId":475742},{"sourceId":618039,"sourceType":"modelInstanceVersion","modelInstanceId":464710,"modelId":480531},{"sourceId":618161,"sourceType":"modelInstanceVersion","modelInstanceId":464813,"modelId":480639},{"sourceId":618749,"sourceType":"modelInstanceVersion","modelInstanceId":465305,"modelId":481145},{"sourceId":619484,"sourceType":"modelInstanceVersion","modelInstanceId":465928,"modelId":481754},{"sourceId":625473,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":470764,"modelId":486662},{"sourceId":625854,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":471094,"modelId":487000}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kescovar/fork-of-dsn-few-shot-learning-resnet50-analysis?scriptVersionId=278718213\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models, datasets, transforms\nfrom torch.utils.data import DataLoader, Subset\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nfrom PIL import Image\nfrom collections import defaultdict\nimport cv2\n\nprint('PyTorch version:', torch.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:41:29.093008Z","iopub.execute_input":"2025-11-16T13:41:29.093316Z","iopub.status.idle":"2025-11-16T13:41:36.736332Z","shell.execute_reply.started":"2025-11-16T13:41:29.093296Z","shell.execute_reply":"2025-11-16T13:41:36.735675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install kaggle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:41:36.73757Z","iopub.execute_input":"2025-11-16T13:41:36.738035Z","iopub.status.idle":"2025-11-16T13:41:40.638274Z","shell.execute_reply.started":"2025-11-16T13:41:36.738017Z","shell.execute_reply":"2025-11-16T13:41:40.637471Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle kernels output kescovar/fork-of-fork-of-dsn-few-shot-8e0f3d9e8ee -p ~/kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:41:40.639483Z","iopub.execute_input":"2025-11-16T13:41:40.640521Z","iopub.status.idle":"2025-11-16T13:41:41.36423Z","shell.execute_reply.started":"2025-11-16T13:41:40.640478Z","shell.execute_reply":"2025-11-16T13:41:41.363285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_PATH_NORMAL = '/kaggle/input/resnet50-26102025/pytorch/default/1/Resnet50_plant_disease_model_26102025.pth'\nMODEL_PATH_PLUS = '/kaggle/input/resnet50plus-31102025/pytorch/default/1/Resnet50plus_plant_disease_model_31102025.pth'\nMODEL_PATH_FS = '/kaggle/input/resnet50plus-31102025-fromscratch/pytorch/default/1/Resnet50plus_plant_disease_model_31102025_fromsractch.pth'\nDATA_DIR = \"/kaggle/input/plantvillage-dataset/color\"\nDATA_DIR_2 = \"/kaggle/input/datasetold/Datasetold/Train\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nOUTPUT_DIR = './gradcam_outputs'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nN_SUPPORT = 15\nN_QUERY = 10\n\nprint('Device:', DEVICE)\nprint('Model path exists?', os.path.exists(MODEL_PATH_FS))\nprint('Model path exists?', os.path.exists(MODEL_PATH_NORMAL))\nprint('Model path exists?', os.path.exists(MODEL_PATH_PLUS))\nprint('Data dir exists?', os.path.exists(DATA_DIR))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:41:41.366256Z","iopub.execute_input":"2025-11-16T13:41:41.366534Z","iopub.status.idle":"2025-11-16T13:41:41.444908Z","shell.execute_reply.started":"2025-11-16T13:41:41.366512Z","shell.execute_reply":"2025-11-16T13:41:41.444161Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Resnet50","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms\nfrom collections import defaultdict\nimport pandas as pd\nfrom sklearn.metrics import classification_report\n\n\n# CONFIGURATION\n\nTRAIN_CSV = '/kaggle/input/classescomplete/train(4).csv'\nVAL_CSV = '/kaggle/input/classescomplete/val(2).csv'\nTEST_CSV = '/kaggle/input/classescomplete/test(2).csv'\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nN_SUPPORT = 5  # Few-shot: 5 support samples per class\nN_QUERY = 10   # 10 query samples per class\nSUBSPACE_DIM = N_SUPPORT - 1\n\nprint(f\"Device: {DEVICE}\")\nprint(f\"Configuration: {N_SUPPORT}-shot, {N_QUERY} queries per class\")\nprint(f\"Subspace dimensionality: {SUBSPACE_DIM}\")\n\n\n# STEP 1: Load Class Information\n\nprint(\"\\n\" + \"-\"*80)\nprint(\"STEP 1: Loading class splits from training...\")\nprint(\"-\"*80)\n\ndef get_classes_from_csv(csv_path):\n    df = pd.read_csv(csv_path)\n    return sorted(df['label'].unique())\n\ntrain_classes = get_classes_from_csv(TRAIN_CSV)\nval_classes = get_classes_from_csv(VAL_CSV)\ntest_classes = get_classes_from_csv(TEST_CSV)\n\nprint(f\"Training classes: {len(train_classes)}\")\nprint(f\"Validation classes: {len(val_classes)}\")\nprint(f\"Test classes: {len(test_classes)}\")\n\n# Combine all classes for comprehensive evaluation\nall_classes = sorted(set(train_classes + val_classes + test_classes))\nprint(f\"\\nTotal classes to evaluate: {len(all_classes)}\")\nprint(f\"Sample classes: {all_classes[:5]} ...\")\n\n\n# STEP 2: Prepare Dataset for All Classes\n\nprint(\"\\n\" + \"-\"*80)\nprint(\"STEP 2: Preparing dataset...\")\nprint(\"-\"*80)\n\ndata_transforms = transforms.Compose([\n    transforms.Resize((84, 84)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Load full dataset\nfull_dataset = datasets.ImageFolder(DATA_DIR, transform=data_transforms)\n\n# Build mapping: class name -> new index\nclass_to_idx = {cls_name: i for i, cls_name in enumerate(all_classes)}\n\n# Filter dataset to only include our target classes\nfiltered_samples = []\nfor path, cls_idx in full_dataset.samples:\n    cls_name = full_dataset.classes[cls_idx]\n    if cls_name in class_to_idx:\n        new_idx = class_to_idx[cls_name]\n        filtered_samples.append((path, new_idx))\n\n# Update dataset\nfull_dataset.samples = filtered_samples\nfull_dataset.targets = [s[1] for s in filtered_samples]\nfull_dataset.classes = all_classes\nfull_dataset.class_to_idx = class_to_idx\nclass_names = all_classes  # For compatibility with reporting code\n\nprint(f\"Filtered dataset: {len(full_dataset)} images across {len(all_classes)} classes\")\n\n# Verify class distribution\nclass_counts = defaultdict(int)\nfor _, label in full_dataset.samples:\n    class_counts[label] += 1\n\nprint(f\"\\nSample class distribution:\")\nfor cls_idx in sorted(class_counts.keys())[:5]:\n    print(f\"  {all_classes[cls_idx][:40]:40s}: {class_counts[cls_idx]:4d} samples\")\nprint(\"  ...\")\n\n\n# STEP 3: Split into Support and Query Sets\n\nprint(\"\\n\" + \"-\"*80)\nprint(\"STEP 3: Creating support and query split...\")\nprint(\"-\"*80)\n\ndef create_support_query_split(dataset, n_support=5, n_query=10, seed=42):\n    \"\"\"\n    Split dataset into support and query sets for few-shot evaluation.\n    \"\"\"\n    np.random.seed(seed)\n    \n    class_to_indices = defaultdict(list)\n    for idx, (_, label) in enumerate(dataset.samples):\n        class_to_indices[label].append(idx)\n    \n    support_indices = []\n    query_indices = []\n    \n    for cls_idx, indices in class_to_indices.items():\n        indices = np.array(indices)\n        np.random.shuffle(indices)\n        \n        total_needed = n_support + n_query\n        if len(indices) < total_needed:\n            print(f\"Warning: Class {cls_idx} ({all_classes[cls_idx]}) has only {len(indices)} samples\")\n            n_sup = min(n_support, len(indices) // 2)\n            n_qry = len(indices) - n_sup\n        else:\n            n_sup = n_support\n            n_qry = n_query\n        \n        support_indices.extend(indices[:n_sup])\n        query_indices.extend(indices[n_sup:n_sup + n_qry])\n    \n    return support_indices, query_indices\n\nsupport_indices, query_indices = create_support_query_split(\n    full_dataset, n_support=N_SUPPORT, n_query=N_QUERY\n)\n\nprint(f\"Support set: {len(support_indices)} samples ({N_SUPPORT} per class)\")\nprint(f\"Query set: {len(query_indices)} samples ({N_QUERY} per class)\")\n\n# Create data loaders\nsupport_dataset = Subset(full_dataset, support_indices)\nquery_dataset = Subset(full_dataset, query_indices)\nsupport_loader = DataLoader(support_dataset, batch_size=32, shuffle=False)\nquery_loader = DataLoader(query_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:41:41.445546Z","iopub.execute_input":"2025-11-16T13:41:41.445892Z","iopub.status.idle":"2025-11-16T13:42:38.634484Z","shell.execute_reply.started":"2025-11-16T13:41:41.445865Z","shell.execute_reply":"2025-11-16T13:42:38.633819Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def BasicBlock():\n    return 0\n\ndef conv_block(in_channels, out_channels):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(),\n        # nn.Hardswish(),\n        nn.MaxPool2d(2)\n    )\n\nclass ConvNet(nn.Module):\n\n    def __init__(self, x_dim=3, hid_dim=64, z_dim=64):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            conv_block(x_dim, hid_dim),\n            conv_block(hid_dim, hid_dim),\n            conv_block(hid_dim, hid_dim),\n            conv_block(hid_dim, z_dim),\n        )\n        self.out_channels = 1600\n\n    def forward(self, x):\n        x = self.encoder(x)\n        return x.view(x.size(0), -1)\n\nimport torchvision.models as models\n\nclass Resnet50(nn.Module):\n    def __init__(self, \n                 hid_dim=512, \n                 z_dim=256, \n                 pretrained=True,\n                 freeze_early_layers=True,\n                 use_batchnorm=True,\n                 normalize_embeddings=True,\n                 weight_decay=1e-4):\n        super().__init__()\n        self.encoder = models.resnet50(pretrained=pretrained)\n        self.encoder.fc = nn.Identity()\n        \n        if freeze_early_layers:\n            for name, param in self.encoder.named_parameters():\n                if not ('layer4' in name or 'fc' in name):\n                    param.requires_grad = False\n        \n        layers = [nn.Linear(2048, hid_dim)]\n        \n        if use_batchnorm:\n            layers.append(nn.BatchNorm1d(hid_dim))\n        \n        layers.extend([\n            nn.ReLU(inplace=True),\n            nn.Linear(hid_dim, z_dim)\n        ])\n        \n        self.projector = nn.Sequential(*layers)\n        self.normalize_embeddings = normalize_embeddings\n        self.weight_decay = weight_decay\n    \n    def forward(self, x):\n        feat = self.encoder(x)\n        embedding = self.projector(feat)\n        \n        if self.normalize_embeddings:\n            embedding = F.normalize(embedding, p=2, dim=1)\n        \n        return embedding\n\nnum_classes = len(class_names)\nmodel = Resnet50()\nmodel.load_state_dict(torch.load(MODEL_PATH_NORMAL, map_location=DEVICE))\nmodel = model.to(DEVICE)\nmodel.eval()\nprint(\"Model loaded and ready for evaluation.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:42:38.635223Z","iopub.execute_input":"2025-11-16T13:42:38.635444Z","iopub.status.idle":"2025-11-16T13:42:41.408207Z","shell.execute_reply.started":"2025-11-16T13:42:38.635426Z","shell.execute_reply":"2025-11-16T13:42:41.407494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# STEP 5: Define Subspace Projection Module (for reference)\n\nprint(\"\\n\" + \"-\"*80)\nprint(\"STEP 5: Defining subspace projection utilities...\")\nprint(\"-\"*80)\n\nclass Subspace_Projection(nn.Module):\n    def __init__(self, num_dim=2, debug=False, eps=1e-8):\n        super().__init__()\n        self.num_dim = num_dim\n        self.debug = debug\n        self.eps = eps  # Better numerical stability\n\n    def create_subspace(self, supportset_features, class_size, sample_size):\n        # Add validation\n        if sample_size < self.num_dim + 1:\n            self.num_dim = sample_size - 1\n            print(f\"Warning: Reduced subspace dim to {self.num_dim}\")\n        all_hyper_planes = []\n        means = []\n        for ii in range(class_size):\n            all_support = supportset_features[ii]\n            mean_vec = torch.mean(all_support, dim=0)\n            means.append(mean_vec)\n            centered = all_support - mean_vec.unsqueeze(0)\n            uu, s, v = torch.svd(centered.transpose(0, 1).double(), some=False)\n            uu = uu.float()\n            all_hyper_planes.append(uu[:, :self.num_dim])  # limit dimension!\n\n        all_hyper_planes = torch.stack(all_hyper_planes, dim=0)\n        means = torch.stack(means, dim=0)\n        return all_hyper_planes, means\n\n    def projection_metric(self, target_features, hyperplanes, mu):\n        eps = 1e-12\n        device = target_features.device\n        batch_size = target_features.shape[0]\n        class_size = hyperplanes.shape[0]\n\n        similarities = []\n        discriminative_loss = torch.tensor(0.0, device=device)\n\n        for j in range(class_size):\n            h_plane_j = hyperplanes[j].unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n            tf_centered = (target_features - mu[j].expand_as(target_features)).unsqueeze(-1)\n            proj = torch.bmm(h_plane_j, torch.bmm(h_plane_j.transpose(1, 2), tf_centered))\n            proj = torch.squeeze(proj, -1) + mu[j].unsqueeze(0).repeat(batch_size, 1)\n\n            diff = target_features - proj\n            query_loss = -torch.sqrt(torch.sum(diff * diff, dim=-1) + eps)\n            similarities.append(query_loss)\n\n            # discriminative term (reduced)\n            for k in range(class_size):\n                if j != k:\n                    temp = torch.mm(hyperplanes[j].T, hyperplanes[k])\n                    discriminative_loss += torch.sum(temp * temp)\n\n        similarities = torch.stack(similarities, dim=1).to(device)\n        class_size = hyperplanes.shape[0]\n        discriminative_loss = discriminative_loss / (class_size * (class_size - 1) + 1e-6)\n        similarities = similarities / similarities.std(dim=1, keepdim=True).clamp_min(1e-6)\n        return similarities\n\nprint(\"Subspace projection utilities defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:42:41.408946Z","iopub.execute_input":"2025-11-16T13:42:41.409204Z","iopub.status.idle":"2025-11-16T13:42:41.419807Z","shell.execute_reply.started":"2025-11-16T13:42:41.409179Z","shell.execute_reply":"2025-11-16T13:42:41.419267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# STEP 6: Compute Subspaces from Support Set\n\nprint(\"\\n\" + \"-\"*80)\nprint(\"STEP 6: Computing class subspaces from support set...\")\nprint(\"-\"*80)\n\ndef compute_subspaces(model, support_loader, device, subspace_dim=4):\n    \"\"\"\n    Compute subspaces for each class using support samples.\n    Uses SVD on centered embeddings - matches training implementation.\n    \"\"\"\n    class_embeddings = defaultdict(list)\n    class_counts = defaultdict(int)\n    \n    with torch.no_grad():\n        for imgs, labels in support_loader:\n            imgs = imgs.to(device)\n            embeddings = model(imgs)\n            # Note: In training, features are NOT L2 normalized before subspace creation\n            \n            for emb, label in zip(embeddings, labels):\n                class_embeddings[label.item()].append(emb)\n                class_counts[label.item()] += 1\n    \n    all_hyperplanes = []\n    all_means = []\n    class_indices = sorted(class_embeddings.keys())\n    \n    for cls_idx in class_indices:\n        embs = class_embeddings[cls_idx]\n        embs_tensor = torch.stack(embs)  # [num_samples, feature_dim]\n        \n        # Compute mean\n        mean_vec = torch.mean(embs_tensor, dim=0)\n        all_means.append(mean_vec)\n        \n        # Center the embeddings\n        centered = embs_tensor - mean_vec.unsqueeze(0)\n        \n        # Validate subspace dimension\n        sample_size = len(embs)\n        num_dim = subspace_dim\n        if sample_size < num_dim + 1:\n            num_dim = sample_size - 1\n            if cls_idx == class_indices[0]:  # Only warn once\n                print(f\"Warning: Reduced subspace dim to {num_dim} for classes with {sample_size} samples\")\n        \n        # SVD on transposed centered embeddings (matching training code)\n        # centered.T shape: [feature_dim, num_samples]\n        uu, s, v = torch.svd(centered.transpose(0, 1).double(), some=False)\n        uu = uu.float()\n        \n        # Take first num_dim columns as hyperplane basis\n        hyperplane = uu[:, :num_dim]  # [feature_dim, num_dim]\n        all_hyperplanes.append(hyperplane)\n    \n    # Stack into tensors\n    all_hyperplanes = torch.stack(all_hyperplanes, dim=0)  # [num_classes, feature_dim, num_dim]\n    all_means = torch.stack(all_means, dim=0)  # [num_classes, feature_dim]\n    \n    return all_hyperplanes, all_means, class_indices, class_counts\n\nhyperplanes, means, class_indices, support_class_counts = compute_subspaces(\n    model, support_loader, DEVICE, subspace_dim=SUBSPACE_DIM\n)\n\nprint(f\"Computed subspaces for {len(class_indices)} classes\")\nprint(f\"Support samples per class (first 5):\")\nfor i, cls_idx in enumerate(class_indices[:5]):\n    print(f\"  {all_classes[cls_idx][:40]:40s}: {support_class_counts[cls_idx]:2d} samples\")\nprint(\"  ...\")\n\n\n# STEP 7: Evaluate on Query Set\n\nprint(\"\\n\" + \"-\"*80)\nprint(\"STEP 7: Evaluating on query set using reconstruction error...\")\nprint(\"-\"*80)\n\ndef evaluate_dsn(model, query_loader, hyperplanes, means, class_indices, device):\n    \"\"\"\n    Evaluate using DSN method: classify based on subspace projection distance.\n    Matches the training implementation's projection_metric approach exactly.\n    \"\"\"\n    all_preds = []\n    all_targets = []\n    all_probs = []\n    \n    eps = 1e-12\n    \n    with torch.no_grad():\n        for imgs, labels in query_loader:\n            imgs = imgs.to(device)\n            query_embeddings = model(imgs)\n            # Note: In training, query features are NOT L2 normalized\n            \n            batch_size = query_embeddings.shape[0]\n            num_classes = hyperplanes.shape[0]\n            \n            similarities = []\n            \n            # For each class, compute projection distance (matching training exactly)\n            for j in range(num_classes):\n                # Get hyperplane and mean for class j\n                h_plane_j = hyperplanes[j].unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n                # h_plane_j: [batch_size, feature_dim, num_dim]\n                \n                # Center query features\n                tf_centered = (query_embeddings - means[j].expand_as(query_embeddings)).unsqueeze(-1)\n                # tf_centered: [batch_size, feature_dim, 1]\n                \n                # Project onto subspace: proj = H @ (H^T @ x_centered) + mean\n                proj = torch.bmm(h_plane_j, torch.bmm(h_plane_j.transpose(1, 2), tf_centered))\n                proj = torch.squeeze(proj, -1) + means[j].unsqueeze(0).repeat(batch_size, 1)\n                # proj: [batch_size, feature_dim]\n                \n                # Compute reconstruction error (distance from subspace)\n                diff = query_embeddings - proj\n                query_loss = -torch.sqrt(torch.sum(diff * diff, dim=-1) + eps)\n                # Negative distance so higher = better (closer to subspace)\n                \n                similarities.append(query_loss)\n            \n            # Stack similarities: [batch_size, num_classes]\n            similarities = torch.stack(similarities, dim=1).to(device)\n            \n            # Normalize by standard deviation (as in training)\n            similarities = similarities / similarities.std(dim=1, keepdim=True).clamp_min(1e-6)\n            \n            # Higher similarity = better, so use argmax for prediction\n            # Note: predictions are in range [0, num_classes-1] corresponding to class_indices\n            preds_idx = torch.argmax(similarities, dim=1)\n            \n            # Map back to actual class labels\n            preds = torch.tensor([class_indices[p.item()] for p in preds_idx], device=device)\n            \n            # Convert to probabilities\n            probs = torch.softmax(similarities, dim=1)\n            \n            all_preds.append(preds.cpu().numpy())\n            all_targets.append(labels.numpy())\n            all_probs.append(probs.cpu().numpy())\n    \n    return (np.concatenate(all_preds), \n            np.concatenate(all_targets), \n            np.concatenate(all_probs))\n\npreds, targets, probs = evaluate_dsn(\n    model, query_loader, hyperplanes, means, class_indices, DEVICE\n)\n\n\n# STEP 8: Results and Analysis\n\nprint(\"\\n\" + \"-\"*80)\nprint(f\"EVALUATION RESULTS: {N_SUPPORT}-SHOT, {N_QUERY} QUERIES PER CLASS\")\nprint(\"-\"*80)\n\nacc = (preds == targets).mean()\nprint(f\"\\nOverall Query Set Accuracy: {acc*100:.2f}%\")\nprint(f\"Total query samples: {len(targets)}\")\n\n# Per-class accuracy\nclass_correct = defaultdict(int)\nclass_total = defaultdict(int)\nfor pred, target in zip(preds, targets):\n    class_total[target] += 1\n    if pred == target:\n        class_correct[target] += 1\n\nprint(f\"\\n{'Class':<50} {'Accuracy':>10} {'Samples':>8}\")\nprint(\"-\" * 70)\nfor cls_idx in sorted(class_total.keys()):\n    cls_acc = class_correct[cls_idx] / class_total[cls_idx] * 100\n    cls_name = all_classes[cls_idx].replace(\"_\", \" \")\n    cls_name = cls_name[:47] + \"...\" if len(cls_name) > 50 else cls_name\n    print(f\"{cls_name:<50} {cls_acc:>9.1f}% {class_total[cls_idx]:>8d}\")\n\n# Overall statistics\nprint(f\"\\n{'-'*70}\")\nprint(\"SUMMARY STATISTICS\")\nprint(f\"{'-'*70}\")\naccuracies = [class_correct[cls] / class_total[cls] for cls in class_total.keys()]\nprint(f\"Mean per-class accuracy: {np.mean(accuracies)*100:.2f}%\")\nprint(f\"Std per-class accuracy:  {np.std(accuracies)*100:.2f}%\")\nprint(f\"Min per-class accuracy:  {np.min(accuracies)*100:.2f}%\")\nprint(f\"Max per-class accuracy:  {np.max(accuracies)*100:.2f}%\")\n\n# Detailed classification report\nprint(f\"\\n{'-'*70}\")\nprint(\"DETAILED CLASSIFICATION REPORT\")\nprint(f\"{'-'*70}\")\nprint(classification_report(\n    targets, preds, \n    target_names=class_names, \n    zero_division=0\n))\n\nprint(f\"\\n{'-'*70}\")\nprint(\"EVALUATION COMPLETE!\")\nprint(f\"{'-'*70}\")\nprint(f\"Evaluated on {len(all_classes)} classes total\")\nprint(f\"  - Training classes: {len(train_classes)}\")\nprint(f\"  - Validation classes: {len(val_classes)}\")\nprint(f\"  - Test classes: {len(test_classes)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:42:41.420603Z","iopub.execute_input":"2025-11-16T13:42:41.420879Z","iopub.status.idle":"2025-11-16T13:42:46.764429Z","shell.execute_reply.started":"2025-11-16T13:42:41.420856Z","shell.execute_reply":"2025-11-16T13:42:46.763556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(targets, preds)\nplt.figure(figsize=(14, 12))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=class_names, yticklabels=class_names,\n            cbar_kws={'label': 'Count'})\nplt.xlabel(\"Predicted\", fontsize=12)\nplt.ylabel(\"True\", fontsize=12)\nplt.title(f\"Confusion Matrix - {N_SUPPORT}-Shot Classification\", fontsize=14)\nplt.xticks(rotation=45, ha='right', fontsize=8)\nplt.yticks(rotation=0, fontsize=8)\nplt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrix.png'), dpi=150, bbox_inches='tight')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:42:46.765289Z","iopub.execute_input":"2025-11-16T13:42:46.765655Z","iopub.status.idle":"2025-11-16T13:42:51.348264Z","shell.execute_reply.started":"2025-11-16T13:42:46.765625Z","shell.execute_reply":"2025-11-16T13:42:51.347552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\ncorrect_mask = preds == targets\ncorrect_probs = probs[correct_mask, targets[correct_mask]]\nincorrect_probs = probs[~correct_mask, preds[~correct_mask]]\n\naxes[0].hist(correct_probs, bins=50, alpha=0.7, label='Correct', color='green')\naxes[0].hist(incorrect_probs, bins=50, alpha=0.7, label='Incorrect', color='red')\naxes[0].set_xlabel('Confidence')\naxes[0].set_ylabel('Frequency')\naxes[0].set_title('Prediction Confidence Distribution')\naxes[0].legend()\naxes[0].grid(alpha=0.3)\n\nclass_confidences = defaultdict(list)\nfor pred, target, prob in zip(preds, targets, probs):\n    class_confidences[target].append(prob[pred])\n\nclass_mean_conf = {cls: np.mean(confs) for cls, confs in class_confidences.items()}\nsorted_classes = sorted(class_mean_conf.items(), key=lambda x: x[1])\n\nclass_indices = [x[0] for x in sorted_classes]\nclass_confs = [x[1] for x in sorted_classes]\nclass_labels = [class_names[i][:20] for i in class_indices]\n\naxes[1].barh(range(len(class_labels)), class_confs, color='steelblue')\naxes[1].set_yticks(range(len(class_labels)))\naxes[1].set_yticklabels(class_labels, fontsize=8)\naxes[1].set_xlabel('Mean Confidence')\naxes[1].set_title('Mean Prediction Confidence per Class')\naxes[1].grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, 'confidence_analysis.png'), dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"Evaluation complete! Results saved to {OUTPUT_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:42:51.350368Z","iopub.execute_input":"2025-11-16T13:42:51.350586Z","iopub.status.idle":"2025-11-16T13:42:52.66641Z","shell.execute_reply.started":"2025-11-16T13:42:51.350569Z","shell.execute_reply":"2025-11-16T13:42:52.665585Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport torch.nn.functional as F\n\nclass GradCAM:\n    def __init__(self, model, target_layer):\n        self.model = model\n        self.target_layer = target_layer\n        self.gradients = None\n        self.activations = None\n        self._register_hooks()\n\n    def _capture_gradients(self, grad):\n        self.gradients = grad\n\n    def _capture_activations(self, module, input, output):\n        self.activations = output\n        output.register_hook(self._capture_gradients)\n\n    def _register_hooks(self):\n        self.target_layer.register_forward_hook(self._capture_activations)\n\n    def generate_heatmap(self, score, target_class):\n        self.model.zero_grad()\n        score.backward(retain_graph=True)\n\n        pooled_gradients = torch.mean(self.gradients, dim=[0, 2, 3])\n        \n        for i in range(self.activations.shape[1]):\n            self.activations[:, i, :, :] *= pooled_gradients[i]\n            \n        heatmap = torch.mean(self.activations, dim=1).squeeze()\n        heatmap = F.relu(heatmap)\n        heatmap /= (torch.max(heatmap) + 1e-8)\n        \n        return heatmap.cpu().detach().numpy()\n\ndef inverse_normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n    for t, m, s in zip(tensor, mean, std):\n        t.mul_(s).add_(m)\n    return tensor\n\ndef plot_gradcam(original_img, heatmap):\n    heatmap = cv2.resize(heatmap, (original_img.shape[1], original_img.shape[0]))\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    superimposed_img = heatmap * 0.4 + original_img\n    return np.clip(superimposed_img, 0, 255).astype(np.uint8)\n\ndef get_resnet_like_last_conv(model):\n    \"\"\"\n    Prefer the canonical resnet last conv in your encoder if available:\n      model.encoder[7][2].conv3\n    Otherwise return the last Conv2d inside model.encoder.\n    \"\"\"\n    # 1) Try the most-likely explicit path\n    try:\n        # Note: Accessing through model.encoder as defined in the model class\n        candidate = model.encoder.layer4[-1].conv3\n        print(\"Using explicit target layer: model.encoder.layer4[-1].conv3\")\n        return candidate\n    except Exception:\n        pass\n\n    # 2) Fallback: find the last Conv2d inside model.encoder\n    last = None\n    for name, module in model.encoder.named_modules():\n        if isinstance(module, nn.Conv2d):\n            last = (name, module)\n    if last is not None:\n        print(\"Using last Conv2d found in encoder:\", last[0])\n        return last[1]\n\n    # 3) If nothing found, raise an informative error\n    raise RuntimeError(\"Could not find a Conv2d in model.encoder to use for Grad-CAM. \"\n                       \"Please inspect model.encoder structure and choose a conv layer manually.\")\n\nprint(\"Selecting target layer for Grad-CAM...\")\ntarget_layer = get_resnet_like_last_conv(model)\ngrad_cam = GradCAM(model, target_layer)\n\n\nprint(\"Generating Grad-CAM visualizations...\")\n\nimages_to_visualize = []\nvisualized_classes = set()\n# Ensure we have a sample for each class if possible\nunique_labels_in_query = sorted(list(set(targets)))\n\nfor label_to_find in unique_labels_in_query:\n    for i, original_idx in enumerate(query_indices):\n        _, original_label = full_dataset.samples[original_idx]\n        if original_label == label_to_find:\n            images_to_visualize.append(original_idx)\n            break # Found one, move to next class\n\n# Limit to 25 images for the plot\nimages_to_visualize = images_to_visualize[:25]\n\n\nfig, axes = plt.subplots(5, 5, figsize=(20, 25))\naxes = axes.flatten()\n\n\nfor i, ax in enumerate(axes):\n    if i >= len(images_to_visualize):\n        ax.axis('off')\n        continue\n        \n    img_idx = images_to_visualize[i]\n    img_tensor, true_label_idx = full_dataset[img_idx]\n    \n    input_tensor = img_tensor.unsqueeze(0).to(DEVICE)\n    query_embedding = model(input_tensor)\n    \n    # --- START: Corrected logic for DSN prediction and score calculation ---\n    \n    # 1. Find the predicted class by calculating reconstruction error for all subspaces\n    distances = []\n    with torch.no_grad():\n        for j in range(len(class_indices)):\n            h_plane_j = hyperplanes[j]\n            mean_j = means[j]\n            \n            tf_centered = query_embedding - mean_j\n            projection_on_basis = torch.matmul(tf_centered, h_plane_j)\n            reconstructed_centered = torch.matmul(projection_on_basis, h_plane_j.t())\n            \n            residual = tf_centered - reconstructed_centered\n            recon_error = torch.sum(residual.pow(2))\n            distances.append(recon_error.item())\n\n    # The prediction is the class with the minimum reconstruction error (distance)\n    pred_idx_relative = np.argmin(distances)\n    pred_label_idx = class_indices[pred_idx_relative]\n\n    # 2. Calculate the score for Grad-CAM using the predicted class's subspace\n    # This must be done with gradient tracking enabled.\n    h_plane_pred = hyperplanes[pred_idx_relative]\n    mean_pred = means[pred_idx_relative]\n    \n    tf_centered_pred = query_embedding - mean_pred\n    projection_on_basis_pred = torch.matmul(tf_centered_pred, h_plane_pred)\n    reconstructed_centered_pred = torch.matmul(projection_on_basis_pred, h_plane_pred.t())\n    \n    residual_pred = tf_centered_pred - reconstructed_centered_pred\n    \n    # The score is the negative reconstruction error (closer is better, so score is higher)\n    score_for_gradcam = -torch.sum(residual_pred.pow(2))\n\n    # --- END: Corrected logic ---\n\n    heatmap = grad_cam.generate_heatmap(score=score_for_gradcam, target_class=pred_label_idx)\n    \n    img_vis = inverse_normalize(img_tensor.clone()).cpu().numpy().transpose(1, 2, 0)\n    img_vis = np.clip(img_vis * 255, 0, 255).astype(np.uint8)\n    overlay = plot_gradcam(img_vis.copy(), heatmap)\n    \n    ax.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)) # Use cvtColor for correct display\n    ax.axis('off')\n    \n    true_label_name = class_names[true_label_idx].replace(\"_\", \" \").split(\"___\")[-1]\n    pred_label_name = class_names[pred_label_idx].replace(\"_\", \" \").split(\"___\")[-1]\n    is_correct = (pred_label_idx == true_label_idx)\n    title_color = 'green' if is_correct else 'red'\n    \n    title = f\"True: {true_label_name}\\nPred: {pred_label_name}\"\n    ax.set_title(title, color=title_color, fontsize=12, pad=10)\n\nplt.tight_layout(pad=3.0)\nplt.savefig(os.path.join(OUTPUT_DIR, 'gradcam_visualization_dsn.png'), dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nGrad-CAM visualization complete! Results saved to {OUTPUT_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:46:39.306703Z","iopub.execute_input":"2025-11-16T13:46:39.307106Z","iopub.status.idle":"2025-11-16T13:46:46.394857Z","shell.execute_reply.started":"2025-11-16T13:46:39.307074Z","shell.execute_reply":"2025-11-16T13:46:46.393916Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms\nfrom collections import defaultdict\nimport pandas as pd\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nimport cv2\n\n# --- CONFIGURATION (re-using global variables) ---\n\n# DATA_DIR_2 is the target dataset for this evaluation block\n# MODEL_PATH_NORMAL is the target model for this evaluation block\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Few-shot parameters (assuming N_SUPPORT, N_QUERY, SUBSPACE_DIM are already defined globally)\n# If not, uncomment and set them here:\n# N_SUPPORT = 5  # Few-shot: 5 support samples per class\n# N_QUERY = 10   # 10 query samples per class\n# SUBSPACE_DIM = N_SUPPORT - 1\n\n# New parameter for splitting classes into base and novel\n# NOVEL_CLASS_PERCENTAGE = 0.5 # 50% of classes will be considered 'novel' for evaluation\n\nprint(f\"Device: {DEVICE}\")\nprint(f\"Configuration: {N_SUPPORT}-shot, {N_QUERY} queries per class\")\nprint(f\"Subspace dimensionality: {SUBSPACE_DIM}\")\n# print(f\"Novel class percentage for evaluation: {NOVEL_CLASS_PERCENTAGE*100}%\")\n\n\n# --- STEP 1: Load Class Information & Prepare Dataset for DATA_DIR_2 ---\n\nprint(\"\\n\" + \"-\"*80)\nprint(f\"STEP 1: Preparing dataset for DATA_DIR_2: {DATA_DIR_2}...\")\nprint(\"-\"*80)\n\ndata_transforms = transforms.Compose([\n    transforms.Resize((84, 84)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Load full dataset from DATA_DIR_2\nfull_dataset_raw = datasets.ImageFolder(DATA_DIR_2, transform=data_transforms)\n\n# Derive class names and mapping directly from DATA_DIR_2\nall_classes_ds2_names = sorted(full_dataset_raw.classes)\nprint(f\"Found {len(all_classes_ds2_names)} classes in {DATA_DIR_2}\")\n\n# --- MODIFICATION START ---\n# Make all classes from DATA_DIR_2 novel classes as requested by user\nnovel_classes_names = all_classes_ds2_names\nbase_classes_names = [] # No base classes for this scenario\nnum_novel_classes = len(novel_classes_names)\n# --- MODIFICATION END ---\n\nprint(f\"Number of novel classes: {len(novel_classes_names)}\")\nprint(f\"Novel classes: {novel_classes_names}\")\nprint(f\"Number of base classes: {len(base_classes_names)}\")\nprint(f\"Base classes: {base_classes_names}\")\n\n# We will only evaluate on novel classes for few-shot\nclass_to_idx_novel = {cls_name: i for i, cls_name in enumerate(novel_classes_names)}\n\n# Filter dataset to only include novel classes for few-shot evaluation\nfiltered_samples_novel_ds2 = []\nfor path, cls_idx_raw in full_dataset_raw.samples:\n    cls_name = full_dataset_raw.classes[cls_idx_raw]\n    if cls_name in class_to_idx_novel:\n        new_idx = class_to_idx_novel[cls_name]\n        filtered_samples_novel_ds2.append((path, new_idx))\n\n# Create a new dataset object for novel classes only\nfull_dataset_novel_ds2 = datasets.ImageFolder(DATA_DIR_2, transform=data_transforms) # Reinitialize to avoid modifying raw\nfull_dataset_novel_ds2.samples = filtered_samples_novel_ds2\nfull_dataset_novel_ds2.targets = [s[1] for s in filtered_samples_novel_ds2]\nfull_dataset_novel_ds2.classes = novel_classes_names\nfull_dataset_novel_ds2.class_to_idx = class_to_idx_novel\n\nclass_names_ds2 = novel_classes_names # Update class_names for this block to reflect novel classes\n\nprint(f\"Filtered dataset for novel classes: {len(full_dataset_novel_ds2)} images across {len(novel_classes_names)} classes\")\n\n# Verify class distribution for novel classes\nclass_counts_novel_ds2 = defaultdict(int)\nfor _, label in full_dataset_novel_ds2.samples:\n    class_counts_novel_ds2[label] += 1\n\nprint(f\"\\nSample class distribution for novel classes:\")\nfor cls_idx in sorted(class_counts_novel_ds2.keys()):\n    print(f\"  {novel_classes_names[cls_idx][:40]:40s}: {class_counts_novel_ds2[cls_idx]:4d} samples\")\nprint(\"  ...\")\n\n\n# --- STEP 2: Split into Support and Query Sets (for Novel Classes) ---\n\nprint(\"\\n\" + \"-\"*80)\nprint(\"STEP 2: Creating support and query split for novel classes...\")\nprint(\"-\"*80)\n\ndef create_support_query_split_ds2(dataset, n_support=5, n_query=10, seed=42):\n    \"\"\"\n    Split dataset into support and query sets for few-shot evaluation.\n    \"\"\"\n    np.random.seed(seed)\n\n    class_to_indices = defaultdict(list)\n    for idx, (_, label) in enumerate(dataset.samples):\n        class_to_indices[label].append(idx)\n\n    support_indices = []\n    query_indices = []\n\n    for cls_idx, indices in class_to_indices.items():\n        indices = np.array(indices)\n        np.random.shuffle(indices)\n\n        total_needed = n_support + n_query\n        # Ensure we have enough samples for support and query\n        if len(indices) < total_needed:\n            print(f\"Warning: Class {cls_idx} ({dataset.classes[cls_idx]}) has only {len(indices)} samples, need {total_needed}.\")\n            n_sup = min(n_support, len(indices) // 2) # Use at most half for support if not enough\n            n_qry = len(indices) - n_sup # Rest for query\n            if n_qry == 0 and n_sup > 0: # If only one sample left after support, put it in query\n                n_qry = n_sup\n                n_sup = 0\n            elif n_sup == 0 and n_qry == 0 and len(indices) > 0: # if len is 1, make it query\n                 n_qry = len(indices)\n        else:\n            n_sup = n_support\n            n_qry = n_query\n\n        support_indices.extend(indices[:n_sup])\n        query_indices.extend(indices[n_sup:n_sup + n_qry])\n\n    return support_indices, query_indices\n\nsupport_indices_ds2, query_indices_ds2 = create_support_query_split_ds2(\n    full_dataset_novel_ds2, n_support=N_SUPPORT, n_query=N_QUERY\n)\n\nprint(f\"Support set: {len(support_indices_ds2)} samples ({N_SUPPORT} per class)\")\nprint(f\"Query set: {len(query_indices_ds2)} samples ({N_QUERY} per class)\")\n\n# Create data loaders\nsupport_dataset_ds2 = Subset(full_dataset_novel_ds2, support_indices_ds2)\nquery_dataset_ds2 = Subset(full_dataset_novel_ds2, query_indices_ds2)\nsupport_loader_ds2 = DataLoader(support_dataset_ds2, batch_size=32, shuffle=False)\nquery_loader_ds2 = DataLoader(query_dataset_ds2, batch_size=32, shuffle=False)\n\n\n# --- STEP 3: Define ResNet50Plus (from scratch) Model Architecture ---\n\nprint(\"\\n\" + \"-\"*80)\nprint(\"STEP 3: Defining ResNet50Plus model architecture...\")\nprint(\"-\"*80)\n\n# Load the trained model\nnum_classes = len(novel_classes_names)\nmodel_ds2 = Resnet50() # num_classes should be based on novel classes\nmodel_ds2.load_state_dict(torch.load(MODEL_PATH_NORMAL, map_location=DEVICE), strict=False) # strict=False because head is changed\nmodel_ds2 = model_ds2.to(DEVICE)\nmodel_ds2.eval()\nprint(\"ResNet50Plus model loaded successfully!\")\n\n\n# --- STEP 4: Define Subspace Projection Module (for reference and function reuse) ---\n\nprint(\"\\n\" + \"-\"*80)\nprint(\"STEP 4: Subspace projection utilities (re-used from previous block)...\")\nprint(\"-\"*80)\n\n\nprint(f\"Computed subspaces for {len(class_indices)} classes\")\nprint(f\"Support samples per class (first 5):\")\nfor i, cls_idx in enumerate(class_indices_ds2[:5]):\n    print(f\"  {novel_classes_names[cls_idx][:40]:40s}: {support_class_counts_ds2[cls_idx]:2d} samples\")\nprint(\"  ...\")\n\n\n# --- STEP 5: Evaluate on Query Set ---\n\nprint(\"\\n\" + \"-\"*80)\nprint(\"STEP 5: Evaluating on query set using reconstruction error...\")\nprint(\"-\"*80)\n\ndef evaluate_dsn_ds2(model, query_loader, hyperplanes, means, class_indices, device):\n    \"\"\"\n    Evaluate using DSN method: classify based on subspace projection distance.\n    \"\"\"\n    all_preds = []\n    all_targets = []\n    all_probs = []\n\n    eps = 1e-12\n\n    if len(class_indices) == 0 or len(query_loader.dataset) == 0: # Handle empty loaders\n        print(\"No classes or query samples to evaluate. Skipping evaluation.\")\n        return np.array([]), np.array([]), np.array([])\n\n    with torch.no_grad():\n        for imgs, labels in query_loader:\n            imgs = imgs.to(device)\n            query_embeddings = model(imgs)\n\n            batch_size = query_embeddings.shape[0]\n            num_classes = hyperplanes.shape[0]\n\n            similarities = []\n\n            # For each class, compute projection distance\n            for j in range(num_classes):\n                h_plane_j = hyperplanes[j].unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n                tf_centered = (query_embeddings - means[j].expand_as(query_embeddings)).unsqueeze(-1)\n                proj = torch.bmm(h_plane_j, torch.bmm(h_plane_j.transpose(1, 2), tf_centered))\n                proj = torch.squeeze(proj, -1) + means[j].unsqueeze(0).repeat(batch_size, 1)\n\n                diff = query_embeddings - proj\n                query_loss = -torch.sqrt(torch.sum(diff * diff, dim=-1) + eps) # Negative distance\n\n                similarities.append(query_loss)\n\n            similarities = torch.stack(similarities, dim=1).to(device)\n\n            # Normalize by standard deviation (as in training) - ONLY if more than one class\n            if num_classes > 1:\n                similarities = similarities / similarities.std(dim=1, keepdim=True).clamp_min(1e-6)\n\n            # Higher similarity = better, so use argmax for prediction\n            # Note: predictions are in range [0, num_classes-1] corresponding to class_indices\n            preds_idx = torch.argmax(similarities, dim=1)\n\n            # Map back to actual class labels\n            preds = torch.tensor([class_indices[p.item()] for p in preds_idx], device=device)\n\n            # Convert to probabilities\n            probs = torch.softmax(similarities, dim=1)\n\n            all_preds.append(preds.cpu().numpy())\n            all_targets.append(labels.numpy())\n            all_probs.append(probs.cpu().numpy())\n\n    return (np.concatenate(all_preds),\n            np.concatenate(all_targets),\n            np.concatenate(all_probs))\n\npreds_ds2, targets_ds2, probs_ds2 = evaluate_dsn_ds2(\n    model_ds2, query_loader_ds2, hyperplanes_ds2, means_ds2, class_indices_ds2, DEVICE\n)\n\n\n# --- STEP 6: Results and Analysis ---\n\nprint(\"\\n\" + \"-\"*80)\nprint(f\"EVALUATION RESULTS FOR DATA_DIR_2 (NOVEL CLASSES): {N_SUPPORT}-SHOT, {N_QUERY} QUERIES PER CLASS\")\nprint(\"-\"*80)\n\nif len(targets_ds2) == 0:\n    print(\"No query samples were evaluated. Skipping accuracy and report generation.\")\nelse:\n    acc_ds2 = (preds_ds2 == targets_ds2).mean()\n    print(f\"\\nOverall Query Set Accuracy: {acc_ds2*100:.2f}%\")\n    print(f\"Total query samples: {len(targets_ds2)}\")\n\n    # Per-class accuracy\n    class_correct_ds2 = defaultdict(int)\n    class_total_ds2 = defaultdict(int)\n    for pred, target in zip(preds_ds2, targets_ds2):\n        class_total_ds2[target] += 1\n        if pred == target:\n            class_correct_ds2[target] += 1\n\n    print(f\"\\n{'Class':<50} {'Accuracy':>10} {'Samples':>8}\")\n    print(\"-\" * 70)\n    for cls_idx in sorted(class_total_ds2.keys()):\n        cls_acc = class_correct_ds2[cls_idx] / class_total_ds2[cls_idx] * 100\n        cls_name = novel_classes_names[cls_idx].replace(\"_\", \" \")\n        cls_name = cls_name[:47] + \"...\" if len(cls_name) > 50 else cls_name\n        print(f\"{cls_name:<50} {cls_acc:>9.1f}% {class_total_ds2[cls_idx]:>8d}\")\n\n    # Overall statistics\n    print(f\"\\n{'-'*70}\")\n    print(\"SUMMARY STATISTICS\")\n    print(f\"{'-'*70}\")\n    accuracies_ds2 = [class_correct_ds2[cls] / class_total_ds2[cls] for cls in class_total_ds2.keys()]\n    print(f\"Mean per-class accuracy: {np.mean(accuracies_ds2)*100:.2f}%\")\n    print(f\"Std per-class accuracy:  {np.std(accuracies_ds2)*100:.2f}%\")\n    print(f\"Min per-class accuracy:  {np.min(accuracies_ds2)*100:.2f}%\")\n    print(f\"Max per-class accuracy:  {np.max(accuracies_ds2)*100:.2f}%\")\n\n    # Detailed classification report\n    print(f\"\\n{'-'*70}\")\n    print(\"DETAILED CLASSIFICATION REPORT\")\n    print(f\"{'-'*70}\")\n    print(classification_report(\n        targets_ds2, preds_ds2,\n        target_names=class_names_ds2,\n        zero_division=0\n    ))\n\nprint(f\"\\n{'-'*70}\")\nprint(\"EVALUATION COMPLETE FOR DATA_DIR_2 (NOVEL CLASSES)!\")\nprint(f\"{'-'*70}\")\nprint(f\"Evaluated on {len(novel_classes_names)} novel classes total\")\n\n# --- STEP 7: Visualize Results ---\n\nif len(targets_ds2) == 0:\n    print(\"No query samples to visualize. Skipping visualization generation.\")\nelse:\n    print(\"\\n\" + \"-\"*80)\n    print(\"STEP 7: Generating visualizations...\")\n    print(\"-\"*80)\n\n    # Confusion Matrix\n    cm_ds2 = confusion_matrix(targets_ds2, preds_ds2)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm_ds2, annot=True, fmt='d', cmap='Blues',\n                xticklabels=class_names_ds2, yticklabels=class_names_ds2,\n                cbar_kws={'label': 'Count'})\n    plt.xlabel(\"Predicted\", fontsize=12)\n    plt.ylabel(\"True\", fontsize=12)\n    plt.title(f\"Confusion Matrix - {N_SUPPORT}-Shot (DATA_DIR_2 - Novel Classes)\", fontsize=14)\n    plt.xticks(rotation=45, ha='right', fontsize=8)\n    plt.yticks(rotation=0, fontsize=8)\n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrix_ds2_novel.png'), dpi=150, bbox_inches='tight')\n    plt.show()\n\n    # Confidence Analysis\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n    correct_mask_ds2 = preds_ds2 == targets_ds2\n    correct_probs_ds2 = probs_ds2[correct_mask_ds2, targets_ds2[correct_mask_ds2]]\n    incorrect_probs_ds2 = probs_ds2[~correct_mask_ds2, preds_ds2[~correct_mask_ds2]]\n\n    # Only plot if there's data to plot\n    if len(correct_probs_ds2) > 0 and not np.all(np.isnan(correct_probs_ds2)):\n        axes[0].hist(correct_probs_ds2, bins=50, alpha=0.7, label='Correct', color='green')\n    if len(incorrect_probs_ds2) > 0 and not np.all(np.isnan(incorrect_probs_ds2)):\n        axes[0].hist(incorrect_probs_ds2, bins=50, alpha=0.7, label='Incorrect', color='red')\n    \n    axes[0].set_xlabel('Confidence')\n    axes[0].set_ylabel('Frequency')\n    axes[0].set_title('Prediction Confidence Distribution (DATA_DIR_2 - Novel Classes)')\n    axes[0].legend()\n    axes[0].grid(alpha=0.3)\n\n    class_confidences_ds2 = defaultdict(list)\n    for pred, target, prob in zip(preds_ds2, targets_ds2, probs_ds2):\n        class_confidences_ds2[target].append(prob[pred])\n\n    class_mean_conf_ds2 = {cls: np.mean(confs) for cls, confs in class_confidences_ds2.items()}\n    sorted_classes_ds2 = sorted(class_mean_conf_ds2.items(), key=lambda x: x[1])\n\n    class_indices_sorted_ds2 = [x[0] for x in sorted_classes_ds2]\n    class_confs_ds2 = [x[1] for x in sorted_classes_ds2]\n    class_labels_ds2 = [novel_classes_names[i][:20] for i in class_indices_sorted_ds2] # Use novel_classes_names\n\n    axes[1].barh(range(len(class_labels_ds2)), class_confs_ds2, color='steelblue')\n    axes[1].set_yticks(range(len(class_labels_ds2)))\n    axes[1].set_yticklabels(class_labels_ds2, fontsize=8)\n    axes[1].set_xlabel('Mean Confidence')\n    axes[1].set_title('Mean Prediction Confidence per Class (DATA_DIR_2 - Novel Classes)')\n    axes[1].grid(axis='x', alpha=0.3)\n\n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, 'confidence_analysis_ds2_novel.png'), dpi=150, bbox_inches='tight') # Renamed output file\n    plt.show()\n\n    # Grad-CAM Visualization\n    class GradCAM:\n        def __init__(self, model, target_layer):\n            self.model = model\n            self.target_layer = target_layer\n            self.gradients = None\n            self.activations = None\n            self._register_hooks()\n\n        def _capture_gradients(self, grad):\n            self.gradients = grad\n\n        def _capture_activations(self, module, input, output):\n            self.activations = output\n            output.register_hook(self._capture_gradients)\n\n        def _register_hooks(self):\n            self.target_layer.register_forward_hook(self._capture_activations)\n\n        def generate_heatmap(self, score, target_class): # target_class not strictly needed for this DSN version\n            self.model.zero_grad()\n            score.backward(retain_graph=True)\n\n            # --- DEBUG PRINTS START ---\n            # if self.gradients is not None:\n            #     print(f\"  Max gradient: {self.gradients.max().item():.4f}\")\n            # if self.activations is None:\n            #     print(\"  WARNING: Activations became None during Grad-CAM.\")\n            #     return np.zeros((self.target_layer.weight.shape[2], self.target_layer.weight.shape[3])) # Return empty heatmap\n            # --- DEBUG PRINTS END ---\n\n            pooled_gradients = torch.mean(self.gradients, dim=[0, 2, 3])\n\n            for i in range(self.activations.shape[1]):\n                self.activations[:, i, :, :] *= pooled_gradients[i]\n\n            heatmap = torch.mean(self.activations, dim=1).squeeze()\n            # --- DEBUG PRINTS START ---\n            # print(f\"  Raw heatmap (before ReLU) min: {heatmap.min().item():.4f}, max: {heatmap.max().item():.4f}\")\n            # --- DEBUG PRINTS END ---\n            heatmap = F.relu(heatmap)\n            # --- DEBUG PRINTS START ---\n            # print(f\"  ReLU heatmap min: {heatmap.min().item():.4f}, max: {heatmap.max().item():.4f}\")\n            # --- DEBUG PRINTS END ---\n\n            max_val = torch.max(heatmap)\n            if max_val == 0:\n                # --- DEBUG PRINTS START ---\n                # print(\"  WARNING: Heatmap is all zeros. Returning empty heatmap.\")\n                # --- DEBUG PRINTS END ---\n                return np.zeros(heatmap.shape)\n\n            heatmap /= (max_val + 1e-8)\n            # --- DEBUG PRINTS START ---\n            # print(f\"  Normalized heatmap min: {heatmap.min().item():.4f}, max: {heatmap.max().item():.4f}\")\n            # --- DEBUG PRINTS END ---\n\n            return heatmap.cpu().detach().numpy()\n\n    def inverse_normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n        for t, m, s in zip(tensor, mean, std):\n            t.mul_(s).add_(m)\n        return tensor\n\n    def plot_gradcam(original_img, heatmap, alpha=0.7): # Added alpha parameter\n        heatmap_resized = cv2.resize(heatmap, (original_img.shape[1], original_img.shape[0]))\n        heatmap_normalized = np.uint8(255 * heatmap_resized) # heatmap is already 0-1\n        heatmap_colored = cv2.applyColorMap(heatmap_normalized, cv2.COLORMAP_JET)\n\n        # Convert images to float for proper blending\n        original_img_float = original_img.astype(np.float32)\n        heatmap_colored_float = heatmap_colored.astype(np.float32)\n\n        # Blend the images: original_img * (1 - alpha) + heatmap_colored * alpha\n        superimposed_img = original_img_float * (1 - alpha) + heatmap_colored_float * alpha\n        return np.clip(superimposed_img, 0, 255).astype(np.uint8)\n\n    def get_resnet_like_last_conv_fs(model):\n        \"\"\"\n        Finds the last convolutional layer in ResNet50Plus_FromScratch model.\n        The last convolutional layer in this architecture is typically the conv3 in the last Bottleneck block of layer4.\n        \"\"\"\n        try:\n            candidate = model.layer4[-1].conv3\n            print(\"Using explicit target layer: model.layer4[-1].conv3\")\n            return candidate\n        except Exception:\n            pass\n\n        # Fallback: find the last Conv2d\n        last = None\n        for name, module in model.named_modules():\n            if isinstance(module, nn.Conv2d):\n                last = (name, module)\n        if last is not None:\n            print(\"Using last Conv2d found in model:\", last[0])\n            return last[1]\n\n        raise RuntimeError(\"Could not find a Conv2d in model to use for Grad-CAM. \"\n                           \"Please inspect model structure and choose a conv layer manually.\")\n\n    print(\"\\nSelecting target layer for Grad-CAM...\")\n    target_layer_ds2 = get_resnet_like_last_conv_fs(model_ds2)\n    grad_cam_ds2 = GradCAM(model_ds2, target_layer_ds2)\n\n    print(\"Generating Grad-CAM visualizations...\")\n\n    images_to_visualize_ds2 = []\n    visualized_classes_ds2 = set()\n    unique_labels_in_query_ds2 = sorted(list(set(targets_ds2)))\n\n    for label_to_find in unique_labels_in_query_ds2:\n        for i, original_idx in enumerate(query_indices_ds2):\n            _, original_label = full_dataset_novel_ds2.samples[original_idx]\n            if original_label == label_to_find:\n                images_to_visualize_ds2.append(original_idx)\n                break # Found one, move to next class\n\n    # Limit to 25 images for the plot\n    images_to_visualize_ds2 = images_to_visualize_ds2[:25]\n\n\n    fig, axes = plt.subplots(5, 5, figsize=(20, 25))\n    axes = axes.flatten()\n\n    for i, ax in enumerate(axes):\n        if i >= len(images_to_visualize_ds2):\n            ax.axis('off')\n            continue\n\n        img_idx = images_to_visualize_ds2[i]\n        img_tensor, true_label_idx = full_dataset_novel_ds2[img_idx]\n\n        input_tensor = img_tensor.unsqueeze(0).to(DEVICE)\n        query_embedding = model_ds2(input_tensor)\n\n        # Find the predicted class by calculating reconstruction error for all subspaces\n        # Need to re-compute query_embeddings with gradient tracking for Grad-CAM's backward pass\n        # IMPORTANT: query_embedding_grad needs to be re-run with requires_grad=True\n        # for the backward pass to work correctly with GradCAM.\n        # However, model_ds2 is in eval() mode. So, we need to temporarily set it to train()\n        # or manually enable requires_grad for relevant tensors if we want to trace gradients through it.\n        # For simplicity and to match the original GradCAM logic, we assume the model output\n        # can be backpropagated through. The score is derived from query_embedding_grad, which\n        # has gradients enabled as it's directly from model_ds2(input_tensor).\n        query_embedding_grad = model_ds2(img_tensor.unsqueeze(0).to(DEVICE))\n\n        distances = []\n        with torch.no_grad(): # Distances calculation does not require gradients\n            for j in range(len(class_indices_ds2)):\n                h_plane_j = hyperplanes_ds2[j]\n                mean_j = means_ds2[j]\n\n                tf_centered = query_embedding_grad - mean_j # Use query_embedding_grad here\n                projection_on_basis = torch.matmul(tf_centered, h_plane_j)\n                reconstructed_centered = torch.matmul(projection_on_basis, h_plane_j.t())\n\n                residual = tf_centered - reconstructed_centered\n                recon_error = torch.sum(residual.pow(2))\n                distances.append(recon_error.item())\n\n        # The prediction is the class with the minimum reconstruction error (distance)\n        pred_idx_relative = np.argmin(distances)\n        pred_label_idx = class_indices_ds2[pred_idx_relative]\n\n        # Calculate the score for Grad-CAM using the predicted class's subspace\n        h_plane_pred = hyperplanes_ds2[pred_idx_relative]\n        mean_pred = means_ds2[pred_idx_relative]\n\n        tf_centered_pred = query_embedding_grad - mean_pred # Use query_embedding_grad here\n        projection_on_basis_pred = torch.matmul(tf_centered_pred, h_plane_pred)\n        reconstructed_centered_pred = torch.matmul(projection_on_basis_pred, h_plane_pred.t())\n\n        residual_pred = tf_centered_pred - reconstructed_centered_pred\n\n        # The score is the negative reconstruction error (closer is better, so score is higher)\n        score_for_gradcam = -torch.sum(residual_pred.pow(2))\n\n        heatmap = grad_cam_ds2.generate_heatmap(score=score_for_gradcam, target_class=pred_label_idx)\n\n        img_vis = inverse_normalize(img_tensor.clone()).cpu().numpy().transpose(1, 2, 0)\n        img_vis = np.clip(img_vis * 255, 0, 255).astype(np.uint8)\n\n        overlay = plot_gradcam(img_vis.copy(), heatmap, alpha=0.7) # Pass alpha\n\n        ax.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n        ax.axis('off')\n\n        true_label_name = novel_classes_names[true_label_idx].replace(\"_\", \" \")\n        pred_label_name = novel_classes_names[pred_label_idx].replace(\"_\", \" \")\n        is_correct = (pred_label_idx == true_label_idx)\n        title_color = 'green' if is_correct else 'red'\n\n        title = f\"True: {true_label_name}\\nPred: {pred_label_name}\"\n        ax.set_title(title, color=title_color, fontsize=12, pad=10)\n\n    plt.tight_layout(pad=3.0)\n    plt.savefig(os.path.join(OUTPUT_DIR, 'gradcam_visualization_ds2_novel.png'), dpi=150, bbox_inches='tight')\n    plt.show()\n\n    print(f\"\\nGrad-CAM visualization complete! Results saved to {OUTPUT_DIR}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Resnet50Plus","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms\nfrom collections import defaultdict\nimport pandas as pd\nfrom sklearn.metrics import classification_report\n\n\n# CONFIGURATION\n\nTRAIN_CSV = '/kaggle/input/classescomplete/train(4).csv'\nVAL_CSV = '/kaggle/input/classescomplete/val(2).csv'\nTEST_CSV = '/kaggle/input/classescomplete/test(2).csv'\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nN_SUPPORT = 5  # Few-shot: 5 support samples per class\nN_QUERY = 10   # 10 query samples per class\nSUBSPACE_DIM = N_SUPPORT - 1\n\nprint(f\"Device: {DEVICE}\")\nprint(f\"Configuration: {N_SUPPORT}-shot, {N_QUERY} queries per class\")\nprint(f\"Subspace dimensionality: {SUBSPACE_DIM}\")\n\n\n# STEP 1: Load Class Information\n\nprint(\"\\n\" + \"-\"*80)\nprint(\"STEP 1: Loading class splits from training...\")\nprint(\"-\"*80)\n\ndef get_classes_from_csv(csv_path):\n    df = pd.read_csv(csv_path)\n    return sorted(df['label'].unique())\n\ntrain_classes = get_classes_from_csv(TRAIN_CSV)\nval_classes = get_classes_from_csv(VAL_CSV)\ntest_classes = get_classes_from_csv(TEST_CSV)\n\nprint(f\"Training classes: {len(train_classes)}\")\nprint(f\"Validation classes: {len(val_classes)}\")\nprint(f\"Test classes: {len(test_classes)}\")\n\n# Combine all classes for comprehensive evaluation\nall_classes = sorted(set(train_classes + val_classes + test_classes))\nprint(f\"\\nTotal classes to evaluate: {len(all_classes)}\")\nprint(f\"Sample classes: {all_classes[:5]} ...\")\n\n\n# STEP 2: Prepare Dataset for All Classes\n\nprint(\"\\n\" + \"-\"*80)\nprint(\"STEP 2: Preparing dataset...\")\nprint(\"-\"*80)\n\ndata_transforms = transforms.Compose([\n    transforms.Resize((84, 84)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Load full dataset\nfull_dataset = datasets.ImageFolder(DATA_DIR, transform=data_transforms)\n\n# Build mapping: class name -> new index\nclass_to_idx = {cls_name: i for i, cls_name in enumerate(all_classes)}\n\n# Filter dataset to only include our target classes\nfiltered_samples = []\nfor path, cls_idx in full_dataset.samples:\n    cls_name = full_dataset.classes[cls_idx]\n    if cls_name in class_to_idx:\n        new_idx = class_to_idx[cls_name]\n        filtered_samples.append((path, new_idx))\n\n# Update dataset\nfull_dataset.samples = filtered_samples\nfull_dataset.targets = [s[1] for s in filtered_samples]\nfull_dataset.classes = all_classes\nfull_dataset.class_to_idx = class_to_idx\nclass_names = all_classes  # For compatibility with reporting code\n\nprint(f\"Filtered dataset: {len(full_dataset)} images across {len(all_classes)} classes\")\n\n# Verify class distribution\nclass_counts = defaultdict(int)\nfor _, label in full_dataset.samples:\n    class_counts[label] += 1\n\nprint(f\"\\nSample class distribution:\")\nfor cls_idx in sorted(class_counts.keys())[:5]:\n    print(f\"  {all_classes[cls_idx][:40]:40s}: {class_counts[cls_idx]:4d} samples\")\nprint(\"  ...\")\n\n\n# STEP 3: Split into Support and Query Sets\n\nprint(\"\\n\" + \"-\"*80)\nprint(\"STEP 3: Creating support and query split...\")\nprint(\"-\"*80)\n\ndef create_support_query_split(dataset, n_support=5, n_query=10, seed=42):\n    \"\"\"\n    Split dataset into support and query sets for few-shot evaluation.\n    \"\"\"\n    np.random.seed(seed)\n    \n    class_to_indices = defaultdict(list)\n    for idx, (_, label) in enumerate(dataset.samples):\n        class_to_indices[label].append(idx)\n    \n    support_indices = []\n    query_indices = []\n    \n    for cls_idx, indices in class_to_indices.items():\n        indices = np.array(indices)\n        np.random.shuffle(indices)\n        \n        total_needed = n_support + n_query\n        if len(indices) < total_needed:\n            print(f\"Warning: Class {cls_idx} ({all_classes[cls_idx]}) has only {len(indices)} samples\")\n            n_sup = min(n_support, len(indices) // 2)\n            n_qry = len(indices) - n_sup\n        else:\n            n_sup = n_support\n            n_qry = n_query\n        \n        support_indices.extend(indices[:n_sup])\n        query_indices.extend(indices[n_sup:n_sup + n_qry])\n    \n    return support_indices, query_indices\n\nsupport_indices, query_indices = create_support_query_split(\n    full_dataset, n_support=N_SUPPORT, n_query=N_QUERY\n)\n\nprint(f\"Support set: {len(support_indices)} samples ({N_SUPPORT} per class)\")\nprint(f\"Query set: {len(query_indices)} samples ({N_QUERY} per class)\")\n\n# Create data loaders\nsupport_dataset = Subset(full_dataset, support_indices)\nquery_dataset = Subset(full_dataset, query_indices)\nsupport_loader = DataLoader(support_dataset, batch_size=32, shuffle=False)\nquery_loader = DataLoader(query_dataset, batch_size=32, shuffle=False)\n\n\n# STEP 4: Define Model Architecture (matching training)\n\nprint(\"\\n\" + \"-\"*80)\nprint(\"STEP 4: Defining model architecture...\")\nprint(\"-\"*80)\n\n\n\nfrom torchsummary import summary\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch\nimport numpy as np\n\ndef BasicBlock():\n    return 0\n\ndef conv_block(in_channels, out_channels):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(),\n        # nn.Hardswish(),\n        nn.MaxPool2d(2)\n    )\n\nclass ConvNet(nn.Module):\n\n    def __init__(self, x_dim=3, hid_dim=64, z_dim=64):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            conv_block(x_dim, hid_dim),\n            conv_block(hid_dim, hid_dim),\n            conv_block(hid_dim, hid_dim),\n            conv_block(hid_dim, z_dim),\n        )\n        self.out_channels = 1600\n\n    def forward(self, x):\n        x = self.encoder(x)\n        return x.view(x.size(0), -1)\n\nimport torchvision.models as models\nclass Resnet50Plus(nn.Module):\n    def __init__(self, pretrained=True, freeze_bn=True, out_dim=2048):\n        super().__init__()\n\n        base_model = models.resnet50(\n            weights=models.ResNet50_Weights.IMAGENET1K_V1 if pretrained else None\n        )\n\n        self.encoder = nn.Sequential(*list(base_model.children())[:-1])\n        self.out_channels = out_dim\n        \n        self.dropout = nn.Dropout(0.5) # 50% dropout\n\n        self.normalize = True\n\n        if freeze_bn:\n            self._freeze_batchnorm()\n\n    def _freeze_batchnorm(self):\n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()\n                m.weight.requires_grad = False\n                m.bias.requires_grad = False\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = torch.flatten(x, 1)  # -> (B, 2048)\n        \n        # Apply dropout\n        if self.training:\n            x = self.dropout(x)\n            \n        if self.normalize:\n            x = nn.functional.normalize(x, p=2, dim=1)\n\n        return x\n# Load the trained model\nmodel = Resnet50Plus()\nmodel.load_state_dict(torch.load(MODEL_PATH_PLUS, map_location=DEVICE))\nmodel = model.to(DEVICE)\nmodel.eval()\nprint(\"Model loaded successfully!\")\n\n\n# STEP 5: Define Subspace Projection Module (for reference)\n\nprint(\"\\n\" + \"-\"*80)\nprint(\"STEP 5: Defining subspace projection utilities...\")\nprint(\"-\"*80)\n\nclass Subspace_Projection(nn.Module):\n    \"\"\"\n    Reference implementation from training (for comparison).\n    This class is not used directly in evaluation, but the logic is replicated\n    in compute_subspaces() and evaluate_dsn() functions below.\n    \"\"\"\n    def __init__(self, num_dim=2, debug=False, eps=1e-8):\n        super().__init__()\n        self.num_dim = num_dim\n        self.debug = debug\n        self.eps = eps\n\n    def create_subspace(self, supportset_features, class_size, sample_size):\n        if sample_size < self.num_dim + 1:\n            self.num_dim = sample_size - 1\n            print(f\"Warning: Reduced subspace dim to {self.num_dim}\")\n        all_hyper_planes = []\n        means = []\n        for ii in range(class_size):\n            all_support = supportset_features[ii]\n            mean_vec = torch.mean(all_support, dim=0)\n            means.append(mean_vec)\n            centered = all_support - mean_vec.unsqueeze(0)\n            uu, s, v = torch.svd(centered.transpose(0, 1).double(), some=False)\n            uu = uu.float()\n            all_hyper_planes.append(uu[:, :self.num_dim])\n\n        all_hyper_planes = torch.stack(all_hyper_planes, dim=0)\n        means = torch.stack(means, dim=0)\n        return all_hyper_planes, means\n\n    def projection_metric(self, target_features, hyperplanes, mu):\n        eps = 1e-12\n        device = target_features.device\n        batch_size = target_features.shape[0]\n        class_size = hyperplanes.shape[0]\n\n        similarities = []\n        for j in range(class_size):\n            h_plane_j = hyperplanes[j].unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n            tf_centered = (target_features - mu[j].expand_as(target_features)).unsqueeze(-1)\n            proj = torch.bmm(h_plane_j, torch.bmm(h_plane_j.transpose(1, 2), tf_centered))\n            proj = torch.squeeze(proj, -1) + mu[j].unsqueeze(0).repeat(batch_size, 1)\n\n            diff = target_features - proj\n            query_loss = -torch.sqrt(torch.sum(diff * diff, dim=-1) + eps)\n            similarities.append(query_loss)\n\n        similarities = torch.stack(similarities, dim=1).to(device)\n        similarities = similarities / similarities.std(dim=1, keepdim=True).clamp_min(1e-6)\n        return similarities\n\nprint(\"Subspace projection utilities defined\")\n\n\n# STEP 6: Compute Subspaces from Support Set\n\nprint(\"\\n\" + \"-\"*80)\nprint(\"STEP 6: Computing class subspaces from support set...\")\nprint(\"-\"*80)\n\ndef compute_subspaces(model, support_loader, device, subspace_dim=4):\n    \"\"\"\n    Compute subspaces for each class using support samples.\n    Uses SVD on centered embeddings - matches training implementation.\n    \"\"\"\n    class_embeddings = defaultdict(list)\n    class_counts = defaultdict(int)\n    \n    with torch.no_grad():\n        for imgs, labels in support_loader:\n            imgs = imgs.to(device)\n            embeddings = model(imgs)\n            # Note: In training, features are NOT L2 normalized before subspace creation\n            \n            for emb, label in zip(embeddings, labels):\n                class_embeddings[label.item()].append(emb)\n                class_counts[label.item()] += 1\n    \n    all_hyperplanes = []\n    all_means = []\n    class_indices = sorted(class_embeddings.keys())\n    \n    for cls_idx in class_indices:\n        embs = class_embeddings[cls_idx]\n        embs_tensor = torch.stack(embs)  # [num_samples, feature_dim]\n        \n        # Compute mean\n        mean_vec = torch.mean(embs_tensor, dim=0)\n        all_means.append(mean_vec)\n        \n        # Center the embeddings\n        centered = embs_tensor - mean_vec.unsqueeze(0)\n        \n        # Validate subspace dimension\n        sample_size = len(embs)\n        num_dim = subspace_dim\n        if sample_size < num_dim + 1:\n            num_dim = sample_size - 1\n            if cls_idx == class_indices[0]:  # Only warn once\n                print(f\"Warning: Reduced subspace dim to {num_dim} for classes with {sample_size} samples\")\n        \n        # SVD on transposed centered embeddings (matching training code)\n        # centered.T shape: [feature_dim, num_samples]\n        uu, s, v = torch.svd(centered.transpose(0, 1).double(), some=False)\n        uu = uu.float()\n        \n        # Take first num_dim columns as hyperplane basis\n        hyperplane = uu[:, :num_dim]  # [feature_dim, num_dim]\n        all_hyperplanes.append(hyperplane)\n    \n    # Stack into tensors\n    all_hyperplanes = torch.stack(all_hyperplanes, dim=0)  # [num_classes, feature_dim, num_dim]\n    all_means = torch.stack(all_means, dim=0)  # [num_classes, feature_dim]\n    \n    return all_hyperplanes, all_means, class_indices, class_counts\n\nhyperplanes, means, class_indices, support_class_counts = compute_subspaces(\n    model, support_loader, DEVICE, subspace_dim=SUBSPACE_DIM\n)\n\nprint(f\"Computed subspaces for {len(class_indices)} classes\")\nprint(f\"Support samples per class (first 5):\")\nfor i, cls_idx in enumerate(class_indices[:5]):\n    print(f\"  {all_classes[cls_idx][:40]:40s}: {support_class_counts[cls_idx]:2d} samples\")\nprint(\"  ...\")\n\n\n# STEP 7: Evaluate on Query Set\n\nprint(\"\\n\" + \"-\"*80)\nprint(\"STEP 7: Evaluating on query set using reconstruction error...\")\nprint(\"-\"*80)\n\ndef evaluate_dsn(model, query_loader, hyperplanes, means, class_indices, device):\n    \"\"\"\n    Evaluate using DSN method: classify based on subspace projection distance.\n    Matches the training implementation's projection_metric approach exactly.\n    \"\"\"\n    all_preds = []\n    all_targets = []\n    all_probs = []\n    \n    eps = 1e-12\n    \n    with torch.no_grad():\n        for imgs, labels in query_loader:\n            imgs = imgs.to(device)\n            query_embeddings = model(imgs)\n            # Note: In training, query features are NOT L2 normalized\n            \n            batch_size = query_embeddings.shape[0]\n            num_classes = hyperplanes.shape[0]\n            \n            similarities = []\n            \n            # For each class, compute projection distance (matching training exactly)\n            for j in range(num_classes):\n                # Get hyperplane and mean for class j\n                h_plane_j = hyperplanes[j].unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n                # h_plane_j: [batch_size, feature_dim, num_dim]\n                \n                # Center query features\n                tf_centered = (query_embeddings - means[j].expand_as(query_embeddings)).unsqueeze(-1)\n                # tf_centered: [batch_size, feature_dim, 1]\n                \n                # Project onto subspace: proj = H @ (H^T @ x_centered) + mean\n                proj = torch.bmm(h_plane_j, torch.bmm(h_plane_j.transpose(1, 2), tf_centered))\n                proj = torch.squeeze(proj, -1) + means[j].unsqueeze(0).repeat(batch_size, 1)\n                # proj: [batch_size, feature_dim]\n                \n                # Compute reconstruction error (distance from subspace)\n                diff = query_embeddings - proj\n                query_loss = -torch.sqrt(torch.sum(diff * diff, dim=-1) + eps)\n                # Negative distance so higher = better (closer to subspace)\n                \n                similarities.append(query_loss)\n            \n            # Stack similarities: [batch_size, num_classes]\n            similarities = torch.stack(similarities, dim=1).to(device)\n            \n            # Normalize by standard deviation (as in training)\n            similarities = similarities / similarities.std(dim=1, keepdim=True).clamp_min(1e-6)\n            \n            # Higher similarity = better, so use argmax for prediction\n            # Note: predictions are in range [0, num_classes-1] corresponding to class_indices\n            preds_idx = torch.argmax(similarities, dim=1)\n            \n            # Map back to actual class labels\n            preds = torch.tensor([class_indices[p.item()] for p in preds_idx], device=device)\n            \n            # Convert to probabilities\n            probs = torch.softmax(similarities, dim=1)\n            \n            all_preds.append(preds.cpu().numpy())\n            all_targets.append(labels.numpy())\n            all_probs.append(probs.cpu().numpy())\n    \n    return (np.concatenate(all_preds), \n            np.concatenate(all_targets), \n            np.concatenate(all_probs))\n\npreds, targets, probs = evaluate_dsn(\n    model, query_loader, hyperplanes, means, class_indices, DEVICE\n)\n\n\n# STEP 8: Results and Analysis\n\nprint(\"\\n\" + \"-\"*80)\nprint(f\"EVALUATION RESULTS: {N_SUPPORT}-SHOT, {N_QUERY} QUERIES PER CLASS\")\nprint(\"-\"*80)\n\nacc = (preds == targets).mean()\nprint(f\"\\nOverall Query Set Accuracy: {acc*100:.2f}%\")\nprint(f\"Total query samples: {len(targets)}\")\n\n# Per-class accuracy\nclass_correct = defaultdict(int)\nclass_total = defaultdict(int)\nfor pred, target in zip(preds, targets):\n    class_total[target] += 1\n    if pred == target:\n        class_correct[target] += 1\n\nprint(f\"\\n{'Class':<50} {'Accuracy':>10} {'Samples':>8}\")\nprint(\"-\" * 70)\nfor cls_idx in sorted(class_total.keys()):\n    cls_acc = class_correct[cls_idx] / class_total[cls_idx] * 100\n    cls_name = all_classes[cls_idx].replace(\"_\", \" \")\n    cls_name = cls_name[:47] + \"...\" if len(cls_name) > 50 else cls_name\n    print(f\"{cls_name:<50} {cls_acc:>9.1f}% {class_total[cls_idx]:>8d}\")\n\n# Overall statistics\nprint(f\"\\n{'-'*70}\")\nprint(\"SUMMARY STATISTICS\")\nprint(f\"{'-'*70}\")\naccuracies = [class_correct[cls] / class_total[cls] for cls in class_total.keys()]\nprint(f\"Mean per-class accuracy: {np.mean(accuracies)*100:.2f}%\")\nprint(f\"Std per-class accuracy:  {np.std(accuracies)*100:.2f}%\")\nprint(f\"Min per-class accuracy:  {np.min(accuracies)*100:.2f}%\")\nprint(f\"Max per-class accuracy:  {np.max(accuracies)*100:.2f}%\")\n\n# Detailed classification report\nprint(f\"\\n{'-'*70}\")\nprint(\"DETAILED CLASSIFICATION REPORT\")\nprint(f\"{'-'*70}\")\nprint(classification_report(\n    targets, preds, \n    target_names=class_names, \n    zero_division=0\n))\n\nprint(f\"\\n{'-'*70}\")\nprint(\"EVALUATION COMPLETE!\")\nprint(f\"{'-'*70}\")\nprint(f\"Evaluated on {len(all_classes)} classes total\")\nprint(f\"  - Training classes: {len(train_classes)}\")\nprint(f\"  - Validation classes: {len(val_classes)}\")\nprint(f\"  - Test classes: {len(test_classes)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:43:00.08749Z","iopub.status.idle":"2025-11-16T13:43:00.087731Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(targets, preds)\nplt.figure(figsize=(14, 12))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=class_names, yticklabels=class_names,\n            cbar_kws={'label': 'Count'})\nplt.xlabel(\"Predicted\", fontsize=12)\nplt.ylabel(\"True\", fontsize=12)\nplt.title(f\"Confusion Matrix - {N_SUPPORT}-Shot Classification\", fontsize=14)\nplt.xticks(rotation=45, ha='right', fontsize=8)\nplt.yticks(rotation=0, fontsize=8)\nplt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrix.png'), dpi=150, bbox_inches='tight')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:43:00.088855Z","iopub.status.idle":"2025-11-16T13:43:00.089138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\ncorrect_mask = preds == targets\ncorrect_probs = probs[correct_mask, targets[correct_mask]]\nincorrect_probs = probs[~correct_mask, preds[~correct_mask]]\n\naxes[0].hist(correct_probs, bins=50, alpha=0.7, label='Correct', color='green')\naxes[0].hist(incorrect_probs, bins=50, alpha=0.7, label='Incorrect', color='red')\naxes[0].set_xlabel('Confidence')\naxes[0].set_ylabel('Frequency')\naxes[0].set_title('Prediction Confidence Distribution')\naxes[0].legend()\naxes[0].grid(alpha=0.3)\n\nclass_confidences = defaultdict(list)\nfor pred, target, prob in zip(preds, targets, probs):\n    class_confidences[target].append(prob[pred])\n\nclass_mean_conf = {cls: np.mean(confs) for cls, confs in class_confidences.items()}\nsorted_classes = sorted(class_mean_conf.items(), key=lambda x: x[1])\n\nclass_indices = [x[0] for x in sorted_classes]\nclass_confs = [x[1] for x in sorted_classes]\nclass_labels = [class_names[i][:20] for i in class_indices]\n\naxes[1].barh(range(len(class_labels)), class_confs, color='steelblue')\naxes[1].set_yticks(range(len(class_labels)))\naxes[1].set_yticklabels(class_labels, fontsize=8)\naxes[1].set_xlabel('Mean Confidence')\naxes[1].set_title('Mean Prediction Confidence per Class')\naxes[1].grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, 'confidence_analysis.png'), dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"Evaluation complete! Results saved to {OUTPUT_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:43:00.090125Z","iopub.status.idle":"2025-11-16T13:43:00.09043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport torch.nn.functional as F\n\nclass GradCAM:\n    def __init__(self, model, target_layer):\n        self.model = model\n        self.target_layer = target_layer\n        self.gradients = None\n        self.activations = None\n        self._register_hooks()\n\n    def _capture_gradients(self, grad):\n        self.gradients = grad\n\n    def _capture_activations(self, module, input, output):\n        self.activations = output\n        output.register_hook(self._capture_gradients)\n\n    def _register_hooks(self):\n        self.target_layer.register_forward_hook(self._capture_activations)\n\n    def generate_heatmap(self, score, target_class):\n        self.model.zero_grad()\n        score.backward(retain_graph=True)\n\n        pooled_gradients = torch.mean(self.gradients, dim=[0, 2, 3])\n        \n        for i in range(self.activations.shape[1]):\n            self.activations[:, i, :, :] *= pooled_gradients[i]\n            \n        heatmap = torch.mean(self.activations, dim=1).squeeze()\n        heatmap = F.relu(heatmap)\n        heatmap /= (torch.max(heatmap) + 1e-8)\n        \n        return heatmap.cpu().detach().numpy()\n\ndef inverse_normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n    for t, m, s in zip(tensor, mean, std):\n        t.mul_(s).add_(m)\n    return tensor\n\ndef plot_gradcam(original_img, heatmap):\n    heatmap = cv2.resize(heatmap, (original_img.shape[1], original_img.shape[0]))\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    superimposed_img = heatmap * 0.4 + original_img\n    return np.clip(superimposed_img, 0, 255).astype(np.uint8)\n\ndef get_resnet_like_last_conv(model):\n    \"\"\"\n    Finds the last convolutional layer in a ResNet-like model.encoder.\n    \"\"\"\n    # 1) Try the most-likely explicit path for a ResNet inside an 'encoder' sequential\n    try:\n        # models.resnet50() creates layers 0-8. 'layer4' is at index 7.\n        candidate = model.encoder[7][-1].conv3\n        print(\"Using explicit target layer: model.encoder[7][-1].conv3\")\n        return candidate\n    except Exception:\n        pass\n\n    # 2) Fallback: find the last Conv2d inside model.encoder\n    last = None\n    for name, module in model.encoder.named_modules():\n        if isinstance(module, nn.Conv2d):\n            last = (name, module)\n    if last is not None:\n        print(\"Using last Conv2d found in encoder:\", last[0])\n        return last[1]\n\n    # 3) If nothing found, raise an informative error\n    raise RuntimeError(\"Could not find a Conv2d in model.encoder to use for Grad-CAM. \"\n                       \"Please inspect model.encoder structure and choose a conv layer manually.\")\n\nprint(\"Selecting target layer for Grad-CAM...\")\ntarget_layer = get_resnet_like_last_conv(model)\ngrad_cam = GradCAM(model, target_layer)\n\nprint(\"Generating Grad-CAM visualizations...\")\n\nimages_to_visualize = []\nvisualized_classes = set()\n# Ensure we have a sample for each class if possible\nunique_labels_in_query = sorted(list(set(targets)))\n\nfor label_to_find in unique_labels_in_query:\n    for i, original_idx in enumerate(query_indices):\n        _, original_label = full_dataset.samples[original_idx]\n        if original_label == label_to_find:\n            images_to_visualize.append(original_idx)\n            break # Found one, move to next class\n\n# Limit to 25 images for the plot\nimages_to_visualize = images_to_visualize[:25]\n\n\nfig, axes = plt.subplots(5, 5, figsize=(20, 25))\naxes = axes.flatten()\n\nfor i, ax in enumerate(axes):\n    if i >= len(images_to_visualize):\n        ax.axis('off')\n        continue\n        \n    img_idx = images_to_visualize[i]\n    img_tensor, true_label_idx = full_dataset[img_idx]\n    \n    input_tensor = img_tensor.unsqueeze(0).to(DEVICE)\n    query_embedding = model(input_tensor)\n    \n    # --- START: Corrected logic for DSN prediction and score calculation ---\n    \n    # 1. Find the predicted class by calculating reconstruction error for all subspaces\n    distances = []\n    with torch.no_grad():\n        for j in range(len(class_indices)):\n            h_plane_j = hyperplanes[j]\n            mean_j = means[j]\n            \n            tf_centered = query_embedding - mean_j\n            projection_on_basis = torch.matmul(tf_centered, h_plane_j)\n            reconstructed_centered = torch.matmul(projection_on_basis, h_plane_j.t())\n            \n            residual = tf_centered - reconstructed_centered\n            recon_error = torch.sum(residual.pow(2))\n            distances.append(recon_error.item())\n\n    # The prediction is the class with the minimum reconstruction error (distance)\n    pred_idx_relative = np.argmin(distances)\n    pred_label_idx = class_indices[pred_idx_relative]\n\n    # 2. Calculate the score for Grad-CAM using the predicted class's subspace\n    # This must be done with gradient tracking enabled.\n    h_plane_pred = hyperplanes[pred_idx_relative]\n    mean_pred = means[pred_idx_relative]\n    \n    tf_centered_pred = query_embedding - mean_pred\n    projection_on_basis_pred = torch.matmul(tf_centered_pred, h_plane_pred)\n    reconstructed_centered_pred = torch.matmul(projection_on_basis_pred, h_plane_pred.t())\n    \n    residual_pred = tf_centered_pred - reconstructed_centered_pred\n    \n    # The score is the negative reconstruction error (closer is better, so score is higher)\n    score_for_gradcam = -torch.sum(residual_pred.pow(2))\n\n    # --- END: Corrected logic ---\n\n    heatmap = grad_cam.generate_heatmap(score=score_for_gradcam, target_class=pred_label_idx)\n\n    img_vis = inverse_normalize(img_tensor.clone()).cpu().numpy().transpose(1, 2, 0)\n    img_vis = np.clip(img_vis * 255, 0, 255).astype(np.uint8)\n    \n    overlay = plot_gradcam(img_vis.copy(), heatmap)\n    \n    ax.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n    ax.axis('off')\n    \n    true_label_name = class_names[true_label_idx].replace(\"_\", \" \").split(\"___\")[-1]\n    pred_label_name = class_names[pred_label_idx].replace(\"_\", \" \").split(\"___\")[-1]\n    is_correct = (pred_label_idx == true_label_idx)\n    title_color = 'green' if is_correct else 'red'\n    \n    title = f\"True: {true_label_name}\\nPred: {pred_label_name}\"\n    ax.set_title(title, color=title_color, fontsize=12, pad=10)\n\nplt.tight_layout(pad=3.0)\nplt.savefig(os.path.join(OUTPUT_DIR, 'gradcam_visualization_dsn_plus.png'), dpi=150, bbox_inches='tight') # Renamed output file\nplt.show()\n\nprint(f\"\\nGrad-CAM visualization complete! Results saved to {OUTPUT_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:43:00.091752Z","iopub.status.idle":"2025-11-16T13:43:00.092069Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Resnet50Plus From Scratch","metadata":{}},{"cell_type":"code","source":"# CONFIGURATION\n\nTRAIN_CSV = '/kaggle/input/classescomplete/train(4).csv'\nVAL_CSV = '/kaggle/input/classescomplete/val(2).csv'\nTEST_CSV = '/kaggle/input/classescomplete/test(2).csv'\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nN_SUPPORT = 5  # Few-shot: 5 support samples per class\nN_QUERY = 10   # 10 query samples per class\nSUBSPACE_DIM = N_SUPPORT - 1\n\nprint(f\"Device: {DEVICE}\")\nprint(f\"Configuration: {N_SUPPORT}-shot, {N_QUERY} queries per class\")\nprint(f\"Subspace dimensionality: {SUBSPACE_DIM}\")\n\n\n# HELPER FUNCTIONS\n\ndef get_classes_from_csv(csv_path):\n    df = pd.read_csv(csv_path)\n    return sorted(df['label'].unique())\n\ndef prepare_dataset(data_dir, target_classes):\n    \"\"\"\n    Prepare dataset from a directory for the given target classes.\n    \"\"\"\n    data_transforms = transforms.Compose([\n        transforms.Resize((84, 84)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\n    # Load full dataset\n    full_dataset = datasets.ImageFolder(data_dir, transform=data_transforms)\n\n    # Build mapping: class name -> new index\n    class_to_idx = {cls_name: i for i, cls_name in enumerate(target_classes)}\n\n    # Filter dataset to only include our target classes\n    filtered_samples = []\n    for path, cls_idx in full_dataset.samples:\n        cls_name = full_dataset.classes[cls_idx]\n        if cls_name in class_to_idx:\n            new_idx = class_to_idx[cls_name]\n            filtered_samples.append((path, new_idx))\n\n    # Update dataset\n    full_dataset.samples = filtered_samples\n    full_dataset.targets = [s[1] for s in filtered_samples]\n    full_dataset.classes = target_classes\n    full_dataset.class_to_idx = class_to_idx\n\n    return full_dataset\n\ndef create_support_query_split(dataset, class_names, n_support=5, n_query=10, seed=42):\n    \"\"\"\n    Split dataset into support and query sets for few-shot evaluation.\n    \"\"\"\n    np.random.seed(seed)\n\n    class_to_indices = defaultdict(list)\n    for idx, (_, label) in enumerate(dataset.samples):\n        class_to_indices[label].append(idx)\n\n    support_indices = []\n    query_indices = []\n\n    for cls_idx, indices in class_to_indices.items():\n        indices = np.array(indices)\n        np.random.shuffle(indices)\n\n        total_needed = n_support + n_query\n        if len(indices) < total_needed:\n            print(f\"Warning: Class {cls_idx} ({class_names[cls_idx]}) has only {len(indices)} samples\")\n            n_sup = min(n_support, len(indices) // 2)\n            n_qry = len(indices) - n_sup\n        else:\n            n_sup = n_support\n            n_qry = n_query\n\n        support_indices.extend(indices[:n_sup])\n        query_indices.extend(indices[n_sup:n_sup + n_qry])\n\n    return support_indices, query_indices\n\ndef compute_subspaces(model, support_loader, device, subspace_dim=4):\n    \"\"\"\n    Compute subspaces for each class using support samples.\n    Uses SVD on centered embeddings - matches training implementation.\n    \"\"\"\n    class_embeddings = defaultdict(list)\n    class_counts = defaultdict(int)\n\n    with torch.no_grad():\n        for imgs, labels in support_loader:\n            imgs = imgs.to(device)\n            embeddings = model(imgs)\n\n            for emb, label in zip(embeddings, labels):\n                class_embeddings[label.item()].append(emb)\n                class_counts[label.item()] += 1\n\n    all_hyperplanes = []\n    all_means = []\n    class_indices = sorted(class_embeddings.keys())\n\n    for cls_idx in class_indices:\n        embs = class_embeddings[cls_idx]\n        embs_tensor = torch.stack(embs)\n\n        mean_vec = torch.mean(embs_tensor, dim=0)\n        all_means.append(mean_vec)\n\n        centered = embs_tensor - mean_vec.unsqueeze(0)\n\n        sample_size = len(embs)\n        num_dim = subspace_dim\n        if sample_size < num_dim + 1:\n            num_dim = sample_size - 1\n            if cls_idx == class_indices[0]:\n                print(f\"Warning: Reduced subspace dim to {num_dim} for classes with {sample_size} samples\")\n\n        uu, s, v = torch.svd(centered.transpose(0, 1).double(), some=False)\n        uu = uu.float()\n\n        hyperplane = uu[:, :num_dim]\n        all_hyperplanes.append(hyperplane)\n\n    all_hyperplanes = torch.stack(all_hyperplanes, dim=0)\n    all_means = torch.stack(all_means, dim=0)\n\n    return all_hyperplanes, all_means, class_indices, class_counts\n\ndef evaluate_dsn(model, query_loader, hyperplanes, means, class_indices, device):\n    \"\"\"\n    Evaluate using DSN method: classify based on subspace projection distance.\n    \"\"\"\n    all_preds = []\n    all_targets = []\n    all_probs = []\n\n    eps = 1e-12\n\n    with torch.no_grad():\n        for imgs, labels in query_loader:\n            imgs = imgs.to(device)\n            query_embeddings = model(imgs)\n\n            batch_size = query_embeddings.shape[0]\n            num_classes = hyperplanes.shape[0]\n\n            similarities = []\n\n            for j in range(num_classes):\n                h_plane_j = hyperplanes[j].unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n\n                tf_centered = (query_embeddings - means[j].expand_as(query_embeddings)).unsqueeze(-1)\n\n                proj = torch.bmm(h_plane_j, torch.bmm(h_plane_j.transpose(1, 2), tf_centered))\n                proj = torch.squeeze(proj, -1) + means[j].unsqueeze(0).repeat(batch_size, 1)\n\n                diff = query_embeddings - proj\n                query_loss = -torch.sqrt(torch.sum(diff * diff, dim=-1) + eps)\n\n                similarities.append(query_loss)\n\n            similarities = torch.stack(similarities, dim=1).to(device)\n            similarities = similarities / similarities.std(dim=1, keepdim=True).clamp_min(1e-6)\n\n            preds_idx = torch.argmax(similarities, dim=1)\n            preds = torch.tensor([class_indices[p.item()] for p in preds_idx], device=device)\n\n            probs = torch.softmax(similarities, dim=1)\n\n            all_preds.append(preds.cpu().numpy())\n            all_targets.append(labels.numpy())\n            all_probs.append(probs.cpu().numpy())\n\n    return (np.concatenate(all_preds),\n            np.concatenate(all_targets),\n            np.concatenate(all_probs))\n\ndef evaluate_dataset(data_dir, dataset_name, model, device, n_support=5, n_query=10, subspace_dim=4):\n    \"\"\"\n    Complete evaluation pipeline for a single dataset.\n    Returns: dict with preds, targets, probs, class_names, support_indices, query_indices, full_dataset, subspaces, accuracy\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(f\"EVALUATING DATASET: {dataset_name}\")\n    print(\"=\"*80)\n\n    # STEP 1: Load Class Information\n    print(\"\\n\" + \"-\"*80)\n    print(\"STEP 1: Loading class splits...\")\n    print(\"-\"*80)\n\n    train_classes = get_classes_from_csv(TRAIN_CSV)\n    val_classes = get_classes_from_csv(VAL_CSV)\n    test_classes = get_classes_from_csv(TEST_CSV)\n\n    all_classes = sorted(set(train_classes + val_classes + test_classes))\n    print(f\"Total classes to evaluate: {len(all_classes)}\")\n\n    # STEP 2: Prepare Dataset\n    print(\"\\n\" + \"-\"*80)\n    print(\"STEP 2: Preparing dataset...\")\n    print(\"-\"*80)\n\n    full_dataset = prepare_dataset(data_dir, all_classes)\n    class_names = all_classes\n\n    print(f\"Filtered dataset: {len(full_dataset)} images across {len(all_classes)} classes\")\n\n    # Verify class distribution\n    class_counts = defaultdict(int)\n    for _, label in full_dataset.samples:\n        class_counts[label] += 1\n\n    print(f\"\\nSample class distribution:\")\n    for cls_idx in sorted(class_counts.keys())[:5]:\n        print(f\"  {all_classes[cls_idx][:40]:40s}: {class_counts[cls_idx]:4d} samples\")\n    print(\"  ...\")\n\n    # STEP 3: Split into Support and Query Sets\n    print(\"\\n\" + \"-\"*80)\n    print(\"STEP 3: Creating support and query split...\")\n    print(\"-\"*80)\n\n    support_indices, query_indices = create_support_query_split(\n        full_dataset, class_names, n_support=n_support, n_query=n_query\n    )\n\n    print(f\"Support set: {len(support_indices)} samples ({n_support} per class)\")\n    print(f\"Query set: {len(query_indices)} samples ({n_query} per class)\")\n\n    support_dataset = Subset(full_dataset, support_indices)\n    query_dataset = Subset(full_dataset, query_indices)\n    support_loader = DataLoader(support_dataset, batch_size=32, shuffle=False)\n    query_loader = DataLoader(query_dataset, batch_size=32, shuffle=False)\n\n    # STEP 4: Compute Subspaces\n    print(\"\\n\" + \"-\"*80)\n    print(\"STEP 4: Computing class subspaces from support set...\")\n    print(\"-\"*80)\n\n    hyperplanes, means, cls_indices, support_class_counts = compute_subspaces(\n        model, support_loader, device, subspace_dim=subspace_dim\n    )\n\n    print(f\"Computed subspaces for {len(cls_indices)} classes\")\n\n    # STEP 5: Evaluate on Query Set\n    print(\"\\n\" + \"-\"*80)\n    print(\"STEP 5: Evaluating on query set...\")\n    print(\"-\"*80)\n\n    preds, targets, probs = evaluate_dsn(\n        model, query_loader, hyperplanes, means, cls_indices, device\n    )\n\n    # STEP 6: Results and Analysis\n    print(\"\\n\" + \"-\"*80)\n    print(f\"EVALUATION RESULTS: {n_support}-SHOT, {n_query} QUERIES PER CLASS\")\n    print(\"-\"*80)\n\n    acc = (preds == targets).mean()\n    print(f\"\\nOverall Query Set Accuracy: {acc*100:.2f}%\")\n    print(f\"Total query samples: {len(targets)}\")\n\n    # Per-class accuracy\n    class_correct = defaultdict(int)\n    class_total = defaultdict(int)\n    for pred, target in zip(preds, targets):\n        class_total[target] += 1\n        if pred == target:\n            class_correct[target] += 1\n\n    print(f\"\\n{'Class':<50} {'Accuracy':>10} {'Samples':>8}\")\n    print(\"-\" * 70)\n    for cls_idx in sorted(class_total.keys()):\n        cls_acc = class_correct[cls_idx] / class_total[cls_idx] * 100\n        cls_name = all_classes[cls_idx].replace(\"_\", \" \")\n        cls_name = cls_name[:47] + \"...\" if len(cls_name) > 50 else cls_name\n        print(f\"{cls_name:<50} {cls_acc:>9.1f}% {class_total[cls_idx]:>8d}\")\n\n    # Overall statistics\n    print(f\"\\n{'-'*70}\")\n    print(\"SUMMARY STATISTICS\")\n    print(f\"{'-'*70}\")\n    accuracies = [class_correct[cls] / class_total[cls] for cls in class_total.keys()]\n    print(f\"Mean per-class accuracy: {np.mean(accuracies)*100:.2f}%\")\n    print(f\"Std per-class accuracy:  {np.std(accuracies)*100:.2f}%\")\n    print(f\"Min per-class accuracy:  {np.min(accuracies)*100:.2f}%\")\n    print(f\"Max per-class accuracy:  {np.max(accuracies)*100:.2f}%\")\n\n    # Detailed classification report\n    print(f\"\\n{'-'*70}\")\n    print(\"DETAILED CLASSIFICATION REPORT\")\n    print(f\"{'-'*70}\")\n    print(classification_report(\n        targets, preds,\n        target_names=class_names,\n        zero_division=0\n    ))\n\n    # Store subspaces for Grad-CAM\n    subspaces = {}\n    for i, cls_idx in enumerate(cls_indices):\n        subspaces[cls_idx] = {\n            'mean': means[i],\n            'basis': hyperplanes[i]\n        }\n\n    return {\n        'preds': preds,\n        'targets': targets,\n        'probs': probs,\n        'class_names': class_names,\n        'support_indices': support_indices,\n        'query_indices': query_indices,\n        'full_dataset': full_dataset,\n        'hyperplanes': hyperplanes,\n        'means': means,\n        'class_indices': cls_indices,\n        'subspaces': subspaces,\n        'accuracy': acc\n    }\n\n\n# LOAD MODEL\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"LOADING MODEL\")\nprint(\"=\"*80)\n\nclass Bottleneck(nn.Module):\n    def __init__(self, in_ch, mid_ch, stride=1, alpha=0.1):\n        super().__init__()\n        out_ch = mid_ch * 4\n        self.conv1 = nn.Conv2d(in_ch, mid_ch, kernel_size=1, bias=False)\n        self.bn1   = nn.BatchNorm2d(mid_ch)\n        self.conv2 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2   = nn.BatchNorm2d(mid_ch)\n        self.conv3 = nn.Conv2d(mid_ch, out_ch, kernel_size=1, bias=False)\n        self.bn3   = nn.BatchNorm2d(out_ch)\n        self.act   = nn.LeakyReLU(alpha, inplace=True)\n        self.short = nn.Sequential()\n        if stride != 1 or in_ch != out_ch:\n            self.short = nn.Sequential(\n                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_ch)\n            )\n    def forward(self, x):\n        identity = self.short(x) if len(self.short) else x\n        out = self.act(self.bn1(self.conv1(x)))\n        out = self.act(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out = self.act(out + identity)\n        return out\n\nclass ResNet50Plus(nn.Module):\n    def __init__(self, num_classes, alpha=0.1, dropout_p=0.5):\n        super().__init__()\n        self.stem = nn.Sequential(\n            nn.Conv2d(3, 64, 7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(alpha, inplace=True),\n            nn.MaxPool2d(3, stride=2, padding=1)\n        )\n        self.layer1 = self._make_layer(64,   64, blocks=3, stride=1, alpha=alpha)\n        self.layer2 = self._make_layer(256, 128, blocks=4, stride=2, alpha=alpha)\n        self.layer3 = self._make_layer(512, 256, blocks=6, stride=2, alpha=alpha)\n        self.layer4 = self._make_layer(1024,512, blocks=3, stride=2, alpha=alpha)\n        self.avg    = nn.AdaptiveAvgPool2d((1,1))\n        self.head   = nn.Sequential(nn.Flatten(), nn.Dropout(dropout_p), nn.Linear(2048, num_classes))\n\n    def _make_layer(self, in_ch, mid_ch, blocks, stride, alpha):\n        layers = [Bottleneck(in_ch, mid_ch, stride=stride, alpha=alpha)]\n        for _ in range(1, blocks):\n            layers.append(Bottleneck(mid_ch*4, mid_ch, stride=1, alpha=alpha))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.stem(x)\n        x = self.layer1(x); x = self.layer2(x); x = self.layer3(x); x = self.layer4(x)\n        x = self.avg(x); x = self.head(x)\n        return x\n\n# Load the trained model\nmodel = ResNet50Plus(num_classes=512)\nmodel.load_state_dict(torch.load(MODEL_PATH_FS, map_location=DEVICE))\nmodel = model.to(DEVICE)\nmodel.eval()\nprint(\"Model loaded successfully!\")\n\n\n# EVALUATE DATASET 1 (Original PlantVillage Dataset)\nresults_1 = evaluate_dataset(\n    data_dir=DATA_DIR,\n    dataset_name=\"PlantVillage (Original Training Dataset)\",\n    model=model,\n    device=DEVICE,\n    n_support=N_SUPPORT,\n    n_query=N_QUERY,\n    subspace_dim=SUBSPACE_DIM\n)\n\n# EVALUATE DATASET 2 (External Dataset: Hama, Sehat, Virus)\nresults_2 = evaluate_dataset(\n    data_dir=DATA_DIR_2,\n    dataset_name=\"External Dataset (Hama, Sehat, Virus)\",\n    model=model,\n    device=DEVICE,\n    n_support=N_SUPPORT,\n    n_query=N_QUERY,\n    subspace_dim=SUBSPACE_DIM\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"EVALUATION COMPLETE FOR BOTH DATASETS\")\nprint(\"=\"*80)\nprint(f\"Dataset 1 (PlantVillage) Accuracy: {results_1['accuracy']*100:.2f}%\")\nprint(f\"Dataset 2 (External) Accuracy: {results_2['accuracy']*100:.2f}%\")\nprint(\"=\"*80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:43:00.093269Z","iopub.status.idle":"2025-11-16T13:43:00.09361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(targets, preds)\nplt.figure(figsize=(14, 12))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=class_names, yticklabels=class_names,\n            cbar_kws={'label': 'Count'})\nplt.xlabel(\"Predicted\", fontsize=12)\nplt.ylabel(\"True\", fontsize=12)\nplt.title(f\"Confusion Matrix - {N_SUPPORT}-Shot Classification\", fontsize=14)\nplt.xticks(rotation=45, ha='right', fontsize=8)\nplt.yticks(rotation=0, fontsize=8)\nplt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrix.png'), dpi=150, bbox_inches='tight')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:43:00.095155Z","iopub.status.idle":"2025-11-16T13:43:00.095582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\ncorrect_mask = preds == targets\ncorrect_probs = probs[correct_mask, targets[correct_mask]]\nincorrect_probs = probs[~correct_mask, preds[~correct_mask]]\n\naxes[0].hist(correct_probs, bins=50, alpha=0.7, label='Correct', color='green')\naxes[0].hist(incorrect_probs, bins=50, alpha=0.7, label='Incorrect', color='red')\naxes[0].set_xlabel('Confidence')\naxes[0].set_ylabel('Frequency')\naxes[0].set_title('Prediction Confidence Distribution')\naxes[0].legend()\naxes[0].grid(alpha=0.3)\n\nclass_confidences = defaultdict(list)\nfor pred, target, prob in zip(preds, targets, probs):\n    class_confidences[target].append(prob[pred])\n\nclass_mean_conf = {cls: np.mean(confs) for cls, confs in class_confidences.items()}\nsorted_classes = sorted(class_mean_conf.items(), key=lambda x: x[1])\n\nclass_indices = [x[0] for x in sorted_classes]\nclass_confs = [x[1] for x in sorted_classes]\nclass_labels = [class_names[i][:20] for i in class_indices]\n\naxes[1].barh(range(len(class_labels)), class_confs, color='steelblue')\naxes[1].set_yticks(range(len(class_labels)))\naxes[1].set_yticklabels(class_labels, fontsize=8)\naxes[1].set_xlabel('Mean Confidence')\naxes[1].set_title('Mean Prediction Confidence per Class')\naxes[1].grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, 'confidence_analysis.png'), dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"Evaluation complete! Results saved to {OUTPUT_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:43:00.0962Z","iopub.status.idle":"2025-11-16T13:43:00.096439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inspect model head & shapes (copy & run)\nunwrap = model.module if hasattr(model, 'module') else model\nprint(\"Model type:\", unwrap.__class__.__name__)\nprint(\"Model final fc module:\", getattr(unwrap, 'fc', None))\nif getattr(unwrap, 'fc', None) is not None:\n    print(\"-> fc.out_features:\", unwrap.fc.out_features)\n# List last few named modules (to find the head name to target in checkpoints)\nprint(\"\\nLast named modules:\")\nmods = list(unwrap.named_modules())\nfor name, mod in mods[-20:]:\n    print(name, \":\", type(mod).__name__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:43:00.097419Z","iopub.status.idle":"2025-11-16T13:43:00.097759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport torch.nn.functional as F\n\nclass GradCAM:\n    def __init__(self, model, target_layer):\n        self.model = model\n        self.target_layer = target_layer\n        self.gradients = None\n        self.activations = None\n        self._register_hooks()\n\n    def _capture_gradients(self, grad):\n        self.gradients = grad\n\n    def _capture_activations(self, module, input, output):\n        self.activations = output\n        output.register_hook(self._capture_gradients)\n\n    def _register_hooks(self):\n        self.target_layer.register_forward_hook(self._capture_activations)\n\n    def generate_heatmap(self, score, target_class):\n        self.model.zero_grad()\n        score.backward(retain_graph=True)\n\n        pooled_gradients = torch.mean(self.gradients, dim=[0, 2, 3])\n\n        for i in range(self.activations.shape[1]):\n            self.activations[:, i, :, :] *= pooled_gradients[i]\n\n        heatmap = torch.mean(self.activations, dim=1).squeeze()\n        heatmap = F.relu(heatmap)\n        heatmap /= (torch.max(heatmap) + 1e-8)\n\n        return heatmap.cpu().detach().numpy()\n\ndef inverse_normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n    for t, m, s in zip(tensor, mean, std):\n        t.mul_(s).add_(m)\n    return tensor\n\ndef plot_gradcam(original_img, heatmap):\n    heatmap = cv2.resize(heatmap, (original_img.shape[1], original_img.shape[0]))\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    superimposed_img = heatmap * 0.4 + original_img\n    return np.clip(superimposed_img, 0, 255).astype(np.uint8)\n\ndef get_resnet_like_last_conv(model):\n    \"\"\"\n    Get the last convolutional layer for Grad-CAM.\n    \"\"\"\n    try:\n        candidate = model.layer4[-1].conv3\n        print(\"Using explicit target layer: model.layer4[-1].conv3\")\n        return candidate\n    except Exception:\n        pass\n\n    last = None\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Conv2d):\n            last = (name, module)\n    if last is not None:\n        print(\"Using last Conv2d found in model:\", last[0])\n        return last[1]\n\n    raise RuntimeError(\"Could not find a Conv2d in model to use for Grad-CAM.\")\n\ndef generate_gradcam_for_dataset(dataset_results, dataset_name, model, device, output_dir):\n    \"\"\"\n    Generate Grad-CAM visualizations for a dataset.\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(f\"GENERATING GRAD-CAM FOR: {dataset_name}\")\n    print(\"=\"*80)\n\n    # Get target layer and initialize Grad-CAM\n    target_layer = get_resnet_like_last_conv(model)\n    grad_cam = GradCAM(model, target_layer)\n\n    # Get data from results\n    class_names = dataset_results['class_names']\n    query_indices = dataset_results['query_indices']\n    full_dataset = dataset_results['full_dataset']\n    subspaces = dataset_results['subspaces']\n\n    # Select images to visualize (one per class)\n    images_to_visualize = []\n    visualized_classes = set()\n    for idx in query_indices:\n        _, label = full_dataset.samples[idx]\n        if label not in visualized_classes:\n            images_to_visualize.append(idx)\n            visualized_classes.add(label)\n        if len(visualized_classes) == len(class_names):\n            break\n\n    # Calculate grid dimensions\n    n_images = len(images_to_visualize)\n    n_cols = 5\n    n_rows = (n_images + n_cols - 1) // n_cols\n\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n    if n_rows == 1:\n        axes = axes.reshape(1, -1)\n    axes = axes.flatten()\n\n    for i, ax in enumerate(axes):\n        if i >= n_images:\n            ax.axis('off')\n            continue\n\n        img_idx = images_to_visualize[i]\n        img_tensor, true_label_idx = full_dataset[img_idx]\n\n        input_tensor = img_tensor.unsqueeze(0).to(device)\n        query_embedding = model(input_tensor)\n\n        # Compute distances to all subspaces\n        distances = []\n        for cls_idx in sorted(subspaces.keys()):\n            subspace = subspaces[cls_idx]\n            mean = subspace['mean']\n            basis = subspace['basis']\n\n            centered_query = query_embedding - mean\n            projection = torch.matmul(torch.matmul(centered_query, basis), basis.t())\n            residual = centered_query - projection\n            recon_error = torch.sum(residual.pow(2))\n            distances.append(recon_error.item())\n\n        pred_label_idx = np.argmin(distances)\n\n        # Generate Grad-CAM\n        pred_subspace = subspaces[pred_label_idx]\n        centered_query = query_embedding - pred_subspace['mean']\n        projection = torch.matmul(torch.matmul(centered_query, pred_subspace['basis']), pred_subspace['basis'].t())\n        score_for_gradcam = -torch.sum((centered_query - projection).pow(2))\n\n        heatmap = grad_cam.generate_heatmap(score=score_for_gradcam, target_class=pred_label_idx)\n\n        # Prepare image for visualization\n        img_vis = inverse_normalize(img_tensor.clone()).cpu().numpy().transpose(1, 2, 0)\n        img_vis = np.clip(img_vis * 255, 0, 255).astype(np.uint8)\n\n        overlay = plot_gradcam(img_vis.copy(), heatmap)\n\n        ax.imshow(overlay)\n        ax.axis('off')\n\n        true_label_name = class_names[true_label_idx].replace(\"_\", \" \").split(\"___\")[-1]\n        pred_label_name = class_names[pred_label_idx].replace(\"_\", \" \").split(\"___\")[-1]\n        is_correct = (pred_label_idx == true_label_idx)\n        title_color = 'green' if is_correct else 'red'\n\n        title = f\"True: {true_label_name}\\nPred: {pred_label_name}\"\n        ax.set_title(title, color=title_color, fontsize=10, pad=10)\n\n    plt.tight_layout(pad=3.0)\n\n    # Save with dataset-specific name\n    safe_name = dataset_name.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\",\", \"\")\n    save_path = os.path.join(output_dir, f'gradcam_{safe_name}.png')\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.show()\n\n    print(f\"Grad-CAM visualization saved to: {save_path}\")\n\n\n# GENERATE GRAD-CAM FOR BOTH DATASETS\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"GENERATING GRAD-CAM VISUALIZATIONS\")\nprint(\"=\"*80)\n\n# Generate for Dataset 1\ngenerate_gradcam_for_dataset(\n    dataset_results=results_1,\n    dataset_name=\"PlantVillage (Original)\",\n    model=model,\n    device=DEVICE,\n    output_dir=OUTPUT_DIR\n)\n\n# Generate for Dataset 2\ngenerate_gradcam_for_dataset(\n    dataset_results=results_2,\n    dataset_name=\"External Dataset (Hama, Sehat, Virus)\",\n    model=model,\n    device=DEVICE,\n    output_dir=OUTPUT_DIR\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ALL GRAD-CAM VISUALIZATIONS COMPLETE!\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:43:00.099173Z","iopub.status.idle":"2025-11-16T13:43:00.099482Z"}},"outputs":[],"execution_count":null}]}