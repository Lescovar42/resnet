{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 658267,
     "sourceType": "datasetVersion",
     "datasetId": 277323
    }
   ],
   "dockerImageVersionId": 31154,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "import albumentations as A",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:18.019290Z",
     "iopub.execute_input": "2025-10-31T02:27:18.019589Z",
     "iopub.status.idle": "2025-10-31T02:27:18.023220Z",
     "shell.execute_reply.started": "2025-10-31T02:27:18.019567Z",
     "shell.execute_reply": "2025-10-31T02:27:18.022573Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": "!pip install albumentations",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:18.025174Z",
     "iopub.execute_input": "2025-10-31T02:27:18.025424Z",
     "iopub.status.idle": "2025-10-31T02:27:21.266526Z",
     "shell.execute_reply.started": "2025-10-31T02:27:18.025407Z",
     "shell.execute_reply": "2025-10-31T02:27:21.265786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.8)\nRequirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.2.6)\nRequirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.15.3)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.3)\nRequirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.12.0a1)\nRequirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.24)\nRequirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.12.0.88)\nRequirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (3.12.5)\nRequirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (6.5.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\nRequirement already satisfied: pydantic-core==2.37.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.37.2)\nRequirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (4.15.0)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": "import os\nimport shutil\nimport time\nimport pprint\n\nimport torch\nimport torch.nn as nn\nimport torch.autograd.variable as Variable\n\nfrom glob import glob\nfrom math import sqrt\nfrom numpy.random import seed\nfrom numpy.random import randn\nfrom numpy import mean\nfrom scipy.stats import sem\nfrom scipy.stats import t\nimport numpy as np\nfrom collections import OrderedDict\nfrom tqdm import tqdm\nimport torch.optim as optim\n\nclass GaussianNoise(nn.Module):\n    def __init__(self, batch_size, input_shape=(3, 84, 84), std=0.05):\n        super(GaussianNoise, self).__init__()\n        self.shape = (batch_size,) + input_shape\n        self.noise = Variable(torch.zeros(self.shape).cuda())\n        self.std = std\n\n    def forward(self, x, std=0.15):\n        noise = Variable(torch.zeros(x.shape).cuda())\n        noise = noise.data.normal_(0, std=std)\n        return x + noise\n\n\ndef set_gpu(x):\n    os.environ['CUDA_VISIBLE_DEVICES'] = x\n    print('using gpu:', x)\n\n\ndef clone(tensor):\n    \"\"\"Detach and clone a tensor including the ``requires_grad`` attribute.\n\n    Arguments:\n        tensor (torch.Tensor): tensor to clone.\n    \"\"\"\n    cloned = tensor.clone()#tensor.detach().clone()\n    # cloned.requires_grad = tensor.requires_grad\n    # if tensor.grad is not None:\n    #     cloned.grad = clone(tensor.grad)\n    return cloned\n\ndef clone_state_dict(state_dict):\n    \"\"\"Clone a state_dict. If state_dict is from a ``torch.nn.Module``, use ``keep_vars=True``.\n\n    Arguments:\n        state_dict (OrderedDict): the state_dict to clone. Assumes state_dict is not detached from model state.\n    \"\"\"\n    return OrderedDict([(name, clone(param)) for name, param in state_dict.items()])\n\ndef ensure_path(path):\n    if os.path.exists(path):\n        if input('{} exists, remove? ([y]/n)'.format(path)) != 'n':\n            shutil.rmtree(path)\n            os.mkdir(path)\n    else:\n        os.mkdir(path)\n\nclass Averager():\n    def __init__(self):\n        self.n = 0\n        self.v = 0\n\n    def add(self, x):\n        self.v = (self.v * self.n + x) / (self.n + 1)\n        self.n += 1\n\n    def item(self):\n        return self.v\n\ndef count_acc(logits, label):\n    pred = torch.argmax(logits, dim=1)\n    return (pred == label).type(torch.cuda.FloatTensor).mean().item()\n\ndef dot_metric(a, b):\n    return torch.mm(a, b.t())\n\ndef euclidean_metric(a, b):\n    n = a.shape[0]\n    m = b.shape[0]\n    a = a.unsqueeze(1).expand(n, m, -1)\n    b = b.unsqueeze(0).expand(n, m, -1)\n    logits = -((a - b)**2).sum(dim=2)\n    return logits\n\nclass Timer():\n\n    def __init__(self):\n        self.o = time.time()\n\n    def measure(self, p=1):\n        x = (time.time() - self.o) / p\n        x = int(x)\n        if x >= 3600:\n            return '{:.1f}h'.format(x / 3600)\n        if x >= 60:\n            return '{}m'.format(round(x / 60))\n        return '{}s'.format(x)\n\n_utils_pp = pprint.PrettyPrinter()\ndef pprint(x):\n    _utils_pp.pprint(x)\n\ndef l2_loss(pred, label):\n    return ((pred - label)**2).sum() / len(pred) / 2\n\ndef set_protocol(data_path, protocol, test_protocol, subset=None):\n    train = []\n    val = []\n    all_set = ['shn', 'hon', 'clv', 'clk', 'gls', 'scl', 'sci', 'nat', 'shx', 'rel']\n    if subset is not None:\n        train.append(data_path + '/crops_' + subset + '/')\n        val.append(data_path + '/crops_' + subset + '/')\n    if protocol == 'p1':\n        for i in range(3):\n            train.append(data_path + '/crops_' + all_set[i])\n    elif protocol == 'p2':\n        for i in range(3, 6):\n            train.append(data_path + '/crops_' + all_set[i])\n    elif protocol == 'p3':\n        for i in range(6, 8):\n            train.append(data_path + '/crops_' + all_set[i])\n    elif protocol == 'p4':\n        for i in range(8, 10):\n            train.append(data_path + '/crops_' + all_set[i])\n\n    if test_protocol == 'p1':\n        for i in range(3):\n            val.append(data_path + '/crops_' + all_set[i])\n    elif test_protocol == 'p2':\n        for i in range(3, 6):\n            val.append(data_path + '/crops_' + all_set[i])\n    elif test_protocol == 'p3':\n        for i in range(6, 8):\n            val.append(data_path + '/crops_' + all_set[i])\n    elif test_protocol == 'p4':\n        for i in range(8, 10):\n            val.append(data_path + '/crops_' + all_set[i])\n    return train, val\n\ndef flip(x, dim):\n    dim = x.dim() + dim if dim < 0 else dim\n    return x[tuple(slice(None, None) if i != dim\n             else torch.arange(x.size(i)-1, -1, -1).long()\n             for i in range(x.dim()))]\n\ndef perturb(data):\n    randno = np.random.randint(0, 5)\n    if randno == 1:\n        return torch.cat((data, data.flip(3)), dim=0)\n    elif randno == 2: #180\n        return torch.cat((data, data.flip(2)), dim=0)\n    elif randno == 3: #90\n        return torch.cat((data, data.transpose(2,3)), dim=0)\n    else:\n        return torch.cat((data, data.transpose(2, 3).flip(3)), dim=0)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:21.268561Z",
     "iopub.execute_input": "2025-10-31T02:27:21.269114Z",
     "iopub.status.idle": "2025-10-31T02:27:21.289462Z",
     "shell.execute_reply.started": "2025-10-31T02:27:21.269091Z",
     "shell.execute_reply": "2025-10-31T02:27:21.288670Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": "# Data Manager",
   "metadata": {
    "editable": false
   }
  },
  {
   "cell_type": "code",
   "source": "cd '/kaggle/working/'",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:21.290370Z",
     "iopub.execute_input": "2025-10-31T02:27:21.290594Z",
     "iopub.status.idle": "2025-10-31T02:27:21.306400Z",
     "shell.execute_reply.started": "2025-10-31T02:27:21.290577Z",
     "shell.execute_reply": "2025-10-31T02:27:21.305744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "/kaggle/working\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": "# --- Step 1: Gather All Data ---\nprint(\"Scanning all image paths...\")\nall_paths_raw = glob(\"/kaggle/input/plantvillage-dataset/color/*/*.JPG\")\n\n# Filter out problematic Pepper classes\nall_paths = []\nfor path in all_paths_raw:\n    if 'Pepper,_bell___healthy' in path or 'Pepper,_bell___Bacterial_spot' in path:\n        continue  # Skip these classes\n    label = path.split('/')[-2]  # Extract class name from path\n    all_paths.append([path, label])\n\nprint(f\"Found {len(all_paths)} total images after filtering out Pepper classes.\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:21.307278Z",
     "iopub.execute_input": "2025-10-31T02:27:21.308010Z",
     "iopub.status.idle": "2025-10-31T02:27:21.502928Z",
     "shell.execute_reply.started": "2025-10-31T02:27:21.307987Z",
     "shell.execute_reply": "2025-10-31T02:27:21.502058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Scanning all image paths...\nFound 50330 total images after filtering out Pepper classes.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": "import os.path as osp\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n\nleaf_train_augmentation = A.Compose([\n    # spatial\n    A.RandomResizedCrop(size=(84, 84), scale=(0.7, 1.0), ratio=(0.75, 1.33), p=1.0),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.3),\n    A.Rotate(limit=180, p=0.7, border_mode=0),\n    A.Perspective(scale=(0.05, 0.1), p=0.4),\n\n    # color / lighting\n    A.OneOf([\n        # controlled color jitter\n        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02, p=1.0),\n        # HSV shifts: explicit shift limits (degrees for hue, percent for saturation/value)\n        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=1.0),\n        # random brightness/contrast\n        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1.0),\n    ], p=0.9),\n\n    # Apply either gamma, CLAHE or tone curve (kept), but not all every time\n    A.OneOf([\n        A.RandomGamma(gamma_limit=(80,120), p=1.0),\n        A.CLAHE(clip_limit=2.0, tile_grid_size=(8,8), p=1.0),\n        A.RandomToneCurve(p=1.0),\n    ], p=0.6),\n\n    # final normalization + tensor conversion\n    A.Normalize(mean=(0.485, 0.456, 0.406),\n                std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\n\nleaf_smart_augmentation = A.Compose([\n    # Conservative spatial transforms\n    A.Resize(92, 92),  # Slightly larger then crop\n    A.RandomCrop(84, 84, p=1.0),\n    A.HorizontalFlip(p=0.5),\n    A.Rotate(limit=15, p=0.3),  # Small rotations\n    \n    # Color variations that mimic real-world conditions\n    A.OneOf([\n        # Lighting changes\n        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=1.0),\n        # Color temperature variations\n        A.RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=1.0),\n    ], p=0.6),\n    \n    # Mild noise for sensor variations\n    A.GaussNoise(var_limit=(5.0, 15.0), p=0.1),\n    \n    # Focus on preserving texture details\n    A.Sharpen(alpha=(0.05, 0.1), lightness=(0.8, 1.0), p=0.1),  # Very mild sharpening\n    \n    # Normalize\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\n\nleaf_val_augmentation = A.Compose([\n    A.Resize(96, 96),  # Slightly larger\n    A.CenterCrop(84, 84),  # Clean center crop\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\n\nleaf_finetune_augmentation = A.Compose([\n    A.RandomResizedCrop(size=(84, 84), scale=(0.85, 1.0), ratio=(0.9, 1.1), p=1.0),\n    A.HorizontalFlip(p=0.5),\n    A.Rotate(limit=45, p=0.5, border_mode=0),\n    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.7),\n    A.GaussianBlur(blur_limit=(3, 5), p=0.15),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\n\n\n# ========================================\n# UPDATED DATASET CLASS\n# ========================================\nclass UiSmell(Dataset):\n    def __init__(self, setname, img_path, is_aug=False):\n        csv_path = osp.join('/kaggle/working/materials/', setname + '.csv')\n        lines = [x.strip() for x in open(csv_path, 'r').readlines()][1:]\n        self.is_aug = is_aug\n        self.img_path = img_path\n\n        data, label = [], []\n        label_map, label_counter = {}, 0\n\n        for line in lines:\n            name, lbl = line.split(',', 1)\n            lbl_clean = lbl.strip()\n            if lbl_clean not in label_map:\n                label_map[lbl_clean] = label_counter\n                label_counter += 1\n            path = osp.join(img_path, name)\n            data.append(path)\n            label.append(label_map[lbl_clean])\n\n        self.data = data\n        self.label = label\n        self.label_map = label_map\n\n        if is_aug:\n            self.transform = leaf_train_augmentation\n            print(f\"✅ Using STRONG leaf augmentation for {setname}\")\n        else:\n            self.transform = leaf_val_augmentation\n            print(f\"✅ Using VALIDATION augmentation for {setname}\")\n\n        print(f\"Loaded {len(self.data)} samples with {len(self.label_map)} classes for {setname}\")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        path = self.data[index]\n        label = self.label[index]\n\n        # PIL -> NumPy (H,W,C)\n        image = np.array(Image.open(path).convert('RGB'))\n\n        # Apply albumentations\n        augmented = self.transform(image=image)\n        image = augmented['image']  # torch.Tensor from ToTensorV2\n\n        # Ensure float32 (should already be)\n        if not torch.is_floating_point(image):\n            image = image.float()\n\n        return image, label",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:21.504863Z",
     "iopub.execute_input": "2025-10-31T02:27:21.505141Z",
     "iopub.status.idle": "2025-10-31T02:27:21.534703Z",
     "shell.execute_reply.started": "2025-10-31T02:27:21.505125Z",
     "shell.execute_reply": "2025-10-31T02:27:21.534039Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "/tmp/ipykernel_37/8913517.py:56: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n  A.GaussNoise(var_limit=(5.0, 15.0), p=0.1),\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": "# import pandas as pd\n# df = pd.read_csv(\"/kaggle/working/materials/train.csv\")\n# df.label.value_counts()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:21.535463Z",
     "iopub.execute_input": "2025-10-31T02:27:21.535761Z",
     "iopub.status.idle": "2025-10-31T02:27:21.539364Z",
     "shell.execute_reply.started": "2025-10-31T02:27:21.535739Z",
     "shell.execute_reply": "2025-10-31T02:27:21.538721Z"
    },
    "editable": false
   },
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": "# Sampler",
   "metadata": {
    "editable": false
   }
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport numpy as np\n\n\nclass CategoriesSampler():\n\n    def __init__(self, label, n_batch, n_cls, n_per):\n        self.n_batch = n_batch\n        self.n_cls = n_cls\n        self.n_per = n_per\n\n        label = np.array(label)\n        self.m_ind = []\n        for i in range(max(label) + 1):\n            ind = np.argwhere(label == i).reshape(-1)\n            ind = torch.from_numpy(ind)\n            if len(ind) > 4:\n                self.m_ind.append(ind)\n\n    def __len__(self):\n        return self.n_batch\n    \n    def __iter__(self):\n        for i_batch in range(self.n_batch):\n            batch = []\n            classes = torch.randperm(len(self.m_ind))[:self.n_cls]\n            for c in classes:\n                l = self.m_ind[c]\n                pos = torch.randperm(len(l))[:self.n_per]\n                batch.append(l[pos])\n            batch = torch.stack(batch).t().reshape(-1)\n            #for i in range(1000):\n            yield batch",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:21.540040Z",
     "iopub.execute_input": "2025-10-31T02:27:21.540217Z",
     "iopub.status.idle": "2025-10-31T02:27:21.551315Z",
     "shell.execute_reply.started": "2025-10-31T02:27:21.540195Z",
     "shell.execute_reply": "2025-10-31T02:27:21.550662Z"
    }
   },
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": "# Convnet",
   "metadata": {
    "editable": false
   }
  },
  {
   "cell_type": "code",
   "source": "!pip install torchsummary",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:21.552078Z",
     "iopub.execute_input": "2025-10-31T02:27:21.552484Z",
     "iopub.status.idle": "2025-10-31T02:27:24.685624Z",
     "shell.execute_reply.started": "2025-10-31T02:27:21.552467Z",
     "shell.execute_reply": "2025-10-31T02:27:24.684865Z"
    },
    "trusted": true,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: torchsummary in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport numpy as np\n\n\nclass CategoriesSampler():\n\n    def __init__(self, label, n_batch, n_cls, n_per):\n        self.n_batch = n_batch\n        self.n_cls = n_cls\n        self.n_per = n_per\n\n        label = np.array(label)\n        self.m_ind = []\n        for i in range(max(label) + 1):\n            ind = np.argwhere(label == i).reshape(-1)\n            ind = torch.from_numpy(ind)\n            if len(ind) > 4:\n                self.m_ind.append(ind)\n\n    def __len__(self):\n        return self.n_batch\n    \n    def __iter__(self):\n        for i_batch in range(self.n_batch):\n            batch = []\n            classes = torch.randperm(len(self.m_ind))[:self.n_cls]\n            for c in classes:\n                l = self.m_ind[c]\n                pos = torch.randperm(len(l))[:self.n_per]\n                batch.append(l[pos])\n            batch = torch.stack(batch).t().reshape(-1)\n            #for i in range(1000):\n            yield batch",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:24.686627Z",
     "iopub.execute_input": "2025-10-31T02:27:24.686828Z",
     "iopub.status.idle": "2025-10-31T02:27:24.693910Z",
     "shell.execute_reply.started": "2025-10-31T02:27:24.686808Z",
     "shell.execute_reply": "2025-10-31T02:27:24.693111Z"
    }
   },
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": "class Bottleneck(nn.Module):\n    def __init__(self, in_ch, mid_ch, stride=1, alpha=0.1):\n        super().__init__()\n        out_ch = mid_ch * 4\n        self.conv1 = nn.Conv2d(in_ch, mid_ch, kernel_size=1, bias=False)\n        self.bn1   = nn.BatchNorm2d(mid_ch)\n        self.conv2 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2   = nn.BatchNorm2d(mid_ch)\n        self.conv3 = nn.Conv2d(mid_ch, out_ch, kernel_size=1, bias=False)\n        self.bn3   = nn.BatchNorm2d(out_ch)\n        self.act   = nn.LeakyReLU(alpha, inplace=True)\n        self.short = nn.Sequential()\n        if stride != 1 or in_ch != out_ch:\n            self.short = nn.Sequential(\n                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_ch)\n            )\n    def forward(self, x):\n        identity = self.short(x) if len(self.short) else x\n        out = self.act(self.bn1(self.conv1(x)))\n        out = self.act(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out = self.act(out + identity)\n        return out\n\nclass ResNet50Plus(nn.Module):\n    def __init__(self, num_classes, alpha=0.1, dropout_p=0.5):\n        super().__init__()\n        self.stem = nn.Sequential(\n            nn.Conv2d(3, 64, 7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(alpha, inplace=True),\n            nn.MaxPool2d(3, stride=2, padding=1)\n        )\n        self.layer1 = self._make_layer(64,   64, blocks=3, stride=1, alpha=alpha)\n        self.layer2 = self._make_layer(256, 128, blocks=4, stride=2, alpha=alpha)\n        self.layer3 = self._make_layer(512, 256, blocks=6, stride=2, alpha=alpha)\n        self.layer4 = self._make_layer(1024,512, blocks=3, stride=2, alpha=alpha)\n        self.avg    = nn.AdaptiveAvgPool2d((1,1))\n        self.head   = nn.Sequential(nn.Flatten(), nn.Dropout(dropout_p), nn.Linear(2048, num_classes))\n\n    def _make_layer(self, in_ch, mid_ch, blocks, stride, alpha):\n        layers = [Bottleneck(in_ch, mid_ch, stride=stride, alpha=alpha)]\n        for _ in range(1, blocks):\n            layers.append(Bottleneck(mid_ch*4, mid_ch, stride=1, alpha=alpha))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.stem(x)\n        x = self.layer1(x); x = self.layer2(x); x = self.layer3(x); x = self.layer4(x)\n        x = self.avg(x); x = self.head(x)\n        return x\n\n# -------------------\n# MixUp helpers\n# -------------------\ndef mixup_batch(x, y, alpha=0.2):\n    if alpha is None or alpha <= 0:\n        return x, (y, y, 1.0)\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0), device=x.device)\n    x_mix = lam * x + (1 - lam) * x[idx]\n    return x_mix, (y, y[idx], lam)\n\ndef mixup_criterion(ce_loss, preds, targets):\n    y_a, y_b, lam = targets\n    return lam * ce_loss(preds, y_a) + (1 - lam) * ce_loss(preds, y_b)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:24.694874Z",
     "iopub.execute_input": "2025-10-31T02:27:24.695138Z",
     "iopub.status.idle": "2025-10-31T02:27:24.710054Z",
     "shell.execute_reply.started": "2025-10-31T02:27:24.695122Z",
     "shell.execute_reply": "2025-10-31T02:27:24.709480Z"
    }
   },
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": "# Classifier",
   "metadata": {
    "editable": false
   }
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\n\nclass Subspace_Projection(nn.Module):\n    def __init__(self, num_dim=2, debug=False, eps=1e-8):\n        super().__init__()\n        self.num_dim = num_dim\n        self.debug = debug\n        self.eps = eps  # Better numerical stability\n\n    def create_subspace(self, supportset_features, class_size, sample_size):\n        # Add validation\n        if sample_size < self.num_dim + 1:\n            self.num_dim = sample_size - 1\n            print(f\"Warning: Reduced subspace dim to {self.num_dim}\")\n        all_hyper_planes = []\n        means = []\n        for ii in range(class_size):\n            all_support = supportset_features[ii]\n            mean_vec = torch.mean(all_support, dim=0)\n            means.append(mean_vec)\n            centered = all_support - mean_vec.unsqueeze(0)\n            uu, s, v = torch.svd(centered.transpose(0, 1).double(), some=False)\n            uu = uu.float()\n            all_hyper_planes.append(uu[:, :self.num_dim])  # limit dimension!\n\n        all_hyper_planes = torch.stack(all_hyper_planes, dim=0)\n        means = torch.stack(means, dim=0)\n        return all_hyper_planes, means\n\n    def projection_metric(self, target_features, hyperplanes, mu):\n        eps = 1e-12\n        device = target_features.device\n        batch_size = target_features.shape[0]\n        class_size = hyperplanes.shape[0]\n\n        similarities = []\n        discriminative_loss = torch.tensor(0.0, device=device)\n\n        for j in range(class_size):\n            h_plane_j = hyperplanes[j].unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n            tf_centered = (target_features - mu[j].expand_as(target_features)).unsqueeze(-1)\n            proj = torch.bmm(h_plane_j, torch.bmm(h_plane_j.transpose(1, 2), tf_centered))\n            proj = torch.squeeze(proj, -1) + mu[j].unsqueeze(0).repeat(batch_size, 1)\n\n            diff = target_features - proj\n            query_loss = -torch.sqrt(torch.sum(diff * diff, dim=-1) + eps)\n            similarities.append(query_loss)\n\n            # discriminative term (reduced)\n            for k in range(class_size):\n                if j != k:\n                    temp = torch.mm(hyperplanes[j].T, hyperplanes[k])\n                    discriminative_loss += torch.sum(temp * temp)\n\n        similarities = torch.stack(similarities, dim=1).to(device)\n        class_size = hyperplanes.shape[0]\n        discriminative_loss = discriminative_loss / (class_size * (class_size - 1) + 1e-6)\n        similarities = similarities / similarities.std(dim=1, keepdim=True).clamp_min(1e-6)\n\n        # ---- DEBUG ----\n        if self.debug:\n            print(\"[DEBUG] projection_metric DEBUG:\")\n            print(f\"  target_features: mean={target_features.mean():.6f}, std={target_features.std():.6f}\")\n            print(f\"  hyperplanes: mean={hyperplanes.mean():.6f}, std={hyperplanes.std():.6f}\")\n            print(f\"  similarities: mean={similarities.mean():.6f}, std={similarities.std():.6f}, min={similarities.min():.6f}, max={similarities.max():.6f}\")\n            print(f\"  discriminative_loss: {discriminative_loss.item():.6f}\")\n\n        return similarities, discriminative_loss",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:24.710715Z",
     "iopub.execute_input": "2025-10-31T02:27:24.710882Z",
     "iopub.status.idle": "2025-10-31T02:27:24.723040Z",
     "shell.execute_reply.started": "2025-10-31T02:27:24.710869Z",
     "shell.execute_reply": "2025-10-31T02:27:24.722286Z"
    }
   },
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": "# Training",
   "metadata": {
    "editable": false
   }
  },
  {
   "cell_type": "code",
   "source": "import os\nimport torch\nimport argparse\nimport numpy as np\nimport os.path as osp\nfrom datetime import datetime\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\n# SET MODEL NAME\nmodelname = 'ResNet50Plus'\n\nargs = {}\nargs['num_sampler'] = 500\nargs['max-epoch'] = 25\nargs['save-epoch'] = 5\nargs['shot'] = 5\nargs['query'] = 5\nargs['train-way'] = 5\nargs['test-way'] = 5\nargs['data-path'] = ''\nargs['gpu'] = '0'\nargs['lamb'] = 0.5\nargs['lr'] = 1e-4\nargs['weight_decay'] = 1e-4\nargs['subspace-dim'] = args['shot']-1\nset_gpu(args['gpu'])\n\ntxt = str(datetime.now())\ntxt = '_'.join([modelname+f\"-{args['shot']}-{args['lamb']}-{args['lr']}-{args['num_sampler']}\", txt[:4], txt[5:7], txt[8:10], txt[11:13], txt[14:16]])\n\nargs['save-path'] = '/kaggle/working/save/'+txt\n\n# MODEL BUILDER\n# Fixed: Added num_classes parameter (512 for feature extraction in few-shot learning)\nmodel = {\n    'ResNet50Plus': ResNet50Plus(num_classes=512),\n}[modelname].cuda()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:24.723846Z",
     "iopub.execute_input": "2025-10-31T02:27:24.724097Z",
     "iopub.status.idle": "2025-10-31T02:27:24.756022Z",
     "shell.execute_reply.started": "2025-10-31T02:27:24.724074Z",
     "shell.execute_reply": "2025-10-31T02:27:24.754800Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import os\nimport csv\nfrom glob import glob\nimport random\nfrom collections import Counter\nimport re",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:24.756417Z",
     "iopub.status.idle": "2025-10-31T02:27:24.756649Z",
     "shell.execute_reply.started": "2025-10-31T02:27:24.756523Z",
     "shell.execute_reply": "2025-10-31T02:27:24.756532Z"
    },
    "editable": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# --- Step 1: Gather All Data ---\nprint(\"Scanning all image paths...\")\nall_paths_raw = glob(\"/kaggle/input/plantvillage-dataset/color/*/*.JPG\")\n\nprint(f\"Found {len(all_paths)} total images and corrected labels in memory.\")\n\n# --- Step 2: Identify All Unique Classes ---\nall_labels = [label for path, label in all_paths]\nunique_classes = sorted(list(set(all_labels)))\nrandom.seed(42) # for reproducibility\nrandom.shuffle(unique_classes)\nnum_classes = len(unique_classes)\nprint(f\"Found {num_classes} unique classes.\")\n\n# --- Step 3: Split the *Classes* into Disjoint Sets ---\ntrain_split = int(0.7 * num_classes)\nval_split = int(0.15 * num_classes)\n\ntrain_classes = unique_classes[:train_split]\nval_classes = unique_classes[train_split : train_split + val_split]\ntest_classes = unique_classes[train_split + val_split:]\n\nprint(f\"Splitting classes into: {len(train_classes)} train, {len(val_classes)} validation, {len(test_classes)} test.\")\n\n# Sanity check: ensure the class sets are disjoint (no overlap)\nassert len(set(train_classes) & set(val_classes)) == 0\nassert len(set(train_classes) & set(test_classes)) == 0\nassert len(set(val_classes) & set(test_classes)) == 0\nprint(\"Class splits are successfully disjoint.\")\n\n\n# --- Step 4: Create Final Data Lists ---\npaths_train, paths_val, paths_test = [], [], []\n\nfor path, label in all_paths:\n    if label in train_classes:\n        paths_train.append([path, label])\n    elif label in val_classes:\n        paths_val.append([path, label])\n    else:\n        paths_test.append([path, label])\n\nprint(f\"Final data splits (number of images): Train={len(paths_train)}, Val={len(paths_val)}, Test={len(paths_test)}\")\n\n\n# --- Step 5: Write the new CSV files ---\ndata = {'train': paths_train,\n        'test': paths_test,\n        'val': paths_val,\n}\n\nos.makedirs(\"/kaggle/working/materials\", exist_ok=True)\ncsv_files = [\"train.csv\", \"test.csv\", \"val.csv\"]\n\nfor fname in csv_files:\n    path = os.path.join(\"/kaggle/working/materials\", fname)\n    with open(path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.writer(f)\n        writer.writerow([\"filename\", \"label\"])\n        writer.writerows(data[fname[:-4]])\n\nprint(\"\\nCSV files with diverse and corrected data splits created successfully inside 'materials' folder.\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:24.757695Z",
     "iopub.status.idle": "2025-10-31T02:27:24.757958Z",
     "shell.execute_reply.started": "2025-10-31T02:27:24.757845Z",
     "shell.execute_reply": "2025-10-31T02:27:24.757855Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# First, create the datasets\ntrainset = UiSmell('train', '/kaggle/input/plantvillage-dataset/color/', is_aug=True)\nvalset = UiSmell('val', '/kaggle/input/plantvillage-dataset/color/', is_aug=False)\ntestset = UiSmell('test', '/kaggle/input/plantvillage-dataset/color/', is_aug=False)\n\n# Now you can print the classes\nprint(\"=== All Classes and Their Labels ===\")\nfor class_name, label_id in trainset.label_map.items():\n    print(f\"Label {label_id}: {class_name}\")\n\nprint(\"\\n=== Classes in Order ===\")\nsorted_classes = sorted(trainset.label_map.items(), key=lambda x: x[1])\nfor class_name, label_id in sorted_classes:\n    print(f\"{label_id}: {class_name}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:24.759410Z",
     "iopub.status.idle": "2025-10-31T02:27:24.759892Z",
     "shell.execute_reply.started": "2025-10-31T02:27:24.759725Z",
     "shell.execute_reply": "2025-10-31T02:27:24.759740Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# DATA LOADER\ntrainset = UiSmell('train', args['data-path'], is_aug=True)\ntrain_sampler = CategoriesSampler(trainset.label, args['num_sampler'],\n                                  args['train-way'], args['shot'] + args['query'])\ntrain_loader = DataLoader(dataset=trainset, batch_sampler=train_sampler, shuffle=False,)\n\nvalset = UiSmell('val', args['data-path'], is_aug=False)\nval_sampler = CategoriesSampler(valset.label, args['num_sampler'],\n                                args['test-way'], args['shot'] + args['query'])\nval_loader = DataLoader(dataset=valset, batch_sampler=val_sampler, shuffle=False,)\n\ntestset = UiSmell('test', args['data-path'], is_aug=False)\ntest_sampler = CategoriesSampler(testset.label, args['num_sampler'],\n                                args['test-way'], args['shot'] + args['query'])\ntest_loader = DataLoader(dataset=testset, batch_sampler=test_sampler, shuffle=False,)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:24.761250Z",
     "iopub.status.idle": "2025-10-31T02:27:24.761526Z",
     "shell.execute_reply.started": "2025-10-31T02:27:24.761397Z",
     "shell.execute_reply": "2025-10-31T02:27:24.761409Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "label_map = trainset.label_map\nsorted_items = sorted(label_map.items(), key=lambda item: item[1])\n\n# Extract the class names in the now correct order\nclass_names_ordered = [item[0] for item in sorted_items]\n\n# Define the output path for the class names file\noutput_path = '/kaggle/working/materials/classes.txt'\n\n# Write the ordered class names to the file, one per line\nwith open(output_path, 'w') as f:\n    for name in class_names_ordered:\n        f.write(f\"{name}\\n\")\n\nprint(f\"Successfully created classes.txt at: {output_path}\")\nprint(\"\\\\n--- Class Names (in order) ---\")\nfor i, name in enumerate(class_names_ordered):\n    print(f\"{i}: {name}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:24.762559Z",
     "iopub.status.idle": "2025-10-31T02:27:24.762817Z",
     "shell.execute_reply.started": "2025-10-31T02:27:24.762713Z",
     "shell.execute_reply": "2025-10-31T02:27:24.762723Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# run in your notebook\nprint(\"Using transform (is_aug):\", trainset.is_aug)\nsample_path = trainset.data[0]\nimport numpy as np\nfrom PIL import Image\nimg = np.array(Image.open(sample_path).convert('RGB'))\nout = trainset.transform(image=img)\nprint(\"transform output keys:\", out.keys())\nprint(\"image type from transform:\", type(out['image']), \"shape:\", getattr(out['image'], 'shape', None))",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:24.764270Z",
     "iopub.status.idle": "2025-10-31T02:27:24.764572Z",
     "shell.execute_reply.started": "2025-10-31T02:27:24.764417Z",
     "shell.execute_reply": "2025-10-31T02:27:24.764430Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nt = out['image']  # torch.Tensor C,H,W\nimg_vis = t.permute(1,2,0).cpu().numpy()\nmean = np.array([0.485,0.456,0.406])\nstd  = np.array([0.229,0.224,0.225])\nimg_vis = np.clip(img_vis * std + mean, 0, 1)\nplt.imshow(img_vis); plt.axis('off'); plt.title('Augmented image (what model sees)')",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:24.765825Z",
     "iopub.status.idle": "2025-10-31T02:27:24.766381Z",
     "shell.execute_reply.started": "2025-10-31T02:27:24.766213Z",
     "shell.execute_reply": "2025-10-31T02:27:24.766228Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "print(\"albumentations version:\", A.__version__)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:24.767051Z",
     "iopub.status.idle": "2025-10-31T02:27:24.767266Z",
     "shell.execute_reply.started": "2025-10-31T02:27:24.767168Z",
     "shell.execute_reply": "2025-10-31T02:27:24.767177Z"
    },
    "editable": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import os, os.path as osp\nimport json\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom tqdm import tqdm\n\n# Ensure save directory exists\nif not os.path.exists(args['save-path']):\n    os.makedirs(args['save-path'])\n\ndef save_model(name):\n    if not os.path.exists(args['save-path']):\n        os.makedirs(args['save-path'])\n    torch.save(model.state_dict(), osp.join(args['save-path'], name + '.pth'))\n\n# Optimizer / Scheduler\noptimizer = optim.Adam(model.parameters(), lr=args['lr'])\nlr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n\n# Projection module\nprojection_pro = Subspace_Projection(num_dim=args['subspace-dim'], debug=False)\n\n# Training log\ntrlog = {\n    'train_loss': [], 'val_loss': [], 'test_loss': [],\n    'train_acc': [], 'val_acc': [], 'test_acc': [],\n    'max_acc': 0.0, 'max_epoch': 0\n}\n\ntimer = Timer()\npatience = 3\nepochs_no_improve = 0\n\n# MAIN TRAIN LOOP\nfor epoch in range(1, args['max-epoch'] + 1):\n    model.train()\n    shot_num = args['shot'] * 2 if args['shot'] == 1 else args['shot']\n\n    tl = Averager()\n    ta = Averager()\n\n    for i, batch in tqdm(enumerate(train_loader, 1)):\n        data, _ = [_.cuda() for _ in batch]\n\n        if i == 1 and epoch == 1:\n            print(f\"[DEBUG] Epoch {epoch}, Iter {i}\")\n            print(f\"data shape: {data.shape}\")\n            print(f\"train-way={args['train-way']}, shot={args['shot']}, query={args['query']}\")\n\n        p = args['shot'] * args['train-way']\n        qq = p + args['query'] * args['train-way']\n        data_shot, data_query = data[:p], data[p:qq]\n\n        if args['shot'] == 1:\n            data_shot = torch.cat((data_shot, flip(data_shot, 3)), dim=0)\n\n        # Forward through model\n        proto = model(data_shot)\n        proto = proto.reshape(shot_num, args['train-way'], -1)\n        proto = torch.transpose(proto, 0, 1)\n        hyperplanes, mu = projection_pro.create_subspace(proto, args['train-way'], shot_num)\n\n        # Labels\n        label = torch.arange(args['train-way']).repeat(args['query'])\n        label = label.type(torch.cuda.LongTensor)\n\n        # Metric projection\n        query_features = model(data_query)\n        logits, discriminative_loss = projection_pro.projection_metric(query_features, hyperplanes, mu=mu)\n\n        ce_loss = F.cross_entropy(logits, label)\n        loss = ce_loss + args['lamb'] * discriminative_loss\n        acc = count_acc(logits, label)\n\n        tl.add(loss.item())\n        ta.add(acc)\n\n        # if i % 50 == 0:\n        #     print(f\"[DEBUG] Epoch {epoch} Iter {i} | loss={loss.item():.4f}, acc={acc:.4f}\")\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    lr_scheduler.step()\n    print(f'epoch {epoch}, loss={tl.item():.4f} acc={ta.item():.4f}')\n\n    tl = tl.item()\n    ta = ta.item()\n\n    # Skip early validation to save time\n    if epoch % 2 != 0 and epoch < 100:\n        continue\n\n    # VALIDATION PHASE\n    model.eval()\n    vl = Averager()\n    va = Averager()\n\n    for i, batch in tqdm(enumerate(val_loader, 1)):\n        data, _ = [_.cuda() for _ in batch]\n        p = args['shot'] * args['test-way']\n        data_shot, data_query = data[:p], data[p:]\n\n        if args['shot'] == 1:\n            data_shot = torch.cat((data_shot, flip(data_shot, 3)), dim=0)\n\n        with torch.no_grad():\n            proto = model(data_shot)\n            proto = proto.reshape(shot_num, args['test-way'], -1)\n            proto = torch.transpose(proto, 0, 1)\n            hyperplanes, mu = projection_pro.create_subspace(proto, args['test-way'], shot_num)\n            logits, _ = projection_pro.projection_metric(model(data_query), hyperplanes, mu=mu)\n\n        label = torch.arange(args['test-way']).repeat(args['query']).type(torch.cuda.LongTensor)\n        loss = F.cross_entropy(logits, label)\n        acc = count_acc(logits, label)\n\n        vl.add(loss.item())\n        va.add(acc)\n\n    vl = vl.item()\n    va = va.item()\n\n    # Save best\n    if va > trlog['max_acc']:\n        trlog['max_acc'] = va\n        save_model('max-acc')\n        trlog['max_epoch'] = epoch\n        epochs_no_improve = 0\n    else:\n        epochs_no_improve += 1\n\n    print(f'epoch {epoch}, val, loss={vl:.4f} acc={va:.4f} maxacc={trlog[\"max_acc\"]:.4f}')\n\n    trlog['train_loss'].append(tl)\n    trlog['train_acc'].append(ta)\n    trlog['val_loss'].append(vl)\n    trlog['val_acc'].append(va)\n\n    save_model('epoch-last')\n    if epoch % args['save-epoch'] == 0:\n        save_model(f'epoch-{epoch}')\n\n    print(f'ETA:{timer.measure()}/{timer.measure(epoch / args[\"max-epoch\"])}')\n\n    # TEST PHASE\n    tel = Averager()\n    tea = Averager()\n\n    for i, batch in tqdm(enumerate(test_loader, 1)):\n        data, _ = [_.cuda() for _ in batch]\n        p = args['shot'] * args['test-way']\n        data_shot, data_query = data[:p], data[p:]\n\n        if args['shot'] == 1:\n            data_shot = torch.cat((data_shot, flip(data_shot, 3)), dim=0)\n\n        with torch.no_grad():\n            proto = model(data_shot)\n            proto = proto.reshape(shot_num, args['test-way'], -1)\n            proto = torch.transpose(proto, 0, 1)\n            hyperplanes, mu = projection_pro.create_subspace(proto, args['test-way'], shot_num)\n            logits, _ = projection_pro.projection_metric(model(data_query), hyperplanes, mu=mu)\n\n        label = torch.arange(args['test-way']).repeat(args['query']).type(torch.cuda.LongTensor)\n        loss = F.cross_entropy(logits, label)\n        acc = count_acc(logits, label)\n\n        tel.add(loss.item())\n        tea.add(acc)\n\n    tel = tel.item()\n    tea = tea.item()\n    print(f'epoch {epoch}, test, loss={tel:.4f} acc={tea:.4f} maxacc={trlog[\"max_acc\"]:.4f}')\n    \n    trlog['test_loss'].append(tel)\n    trlog['test_acc'].append(tea)\n\n    save_path = osp.join(args['save-path'], 'trlog.json')\n    with open(save_path, 'w') as f:\n        json.dump(trlog, f, indent=4)\n\n    print(f'TEST ETA:{timer.measure()}/{timer.measure(epoch / args[\"max-epoch\"])}')\n\n    if epochs_no_improve >= patience:\n        print(f'Early stopping triggered after {patience} epochs with no improvement.')\n        break",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:24.768690Z",
     "iopub.status.idle": "2025-10-31T02:27:24.768998Z",
     "shell.execute_reply.started": "2025-10-31T02:27:24.768844Z",
     "shell.execute_reply": "2025-10-31T02:27:24.768857Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nimport json\nimport os.path as osp\n\n# Load trlog (if not already in memory)\nsave_path = osp.join(args['save-path'], 'trlog.json')\nwith open(save_path, 'r') as f:\n    trlog = json.load(f)\n\nepochs = range(1, len(trlog['train_loss']) + 1)\n\nplt.figure(figsize=(12, 5))\n\n# ----- LOSS -----\nplt.subplot(1, 2, 1)\nplt.plot(epochs, trlog['train_loss'], label='Train Loss')\nplt.plot(epochs, trlog['val_loss'], label='Val Loss')\nplt.plot(epochs, trlog['test_loss'], label='Test Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss over Epochs')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\n\n# ----- ACCURACY -----\nplt.subplot(1, 2, 2)\nplt.plot(epochs, trlog['train_acc'], label='Train Acc')\nplt.plot(epochs, trlog['val_acc'], label='Val Acc')\nplt.plot(epochs, trlog['test_acc'], label='Test Acc')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Accuracy over Epochs')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:24.770186Z",
     "iopub.status.idle": "2025-10-31T02:27:24.770462Z",
     "shell.execute_reply.started": "2025-10-31T02:27:24.770328Z",
     "shell.execute_reply": "2025-10-31T02:27:24.770341Z"
    },
    "editable": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "save_model_path = '/kaggle/working/Resnet50_plant_disease_model_26102025.pth'\ntorch.save(model.state_dict(), save_model_path)\n\nprint(f\"Model telah disimpan di: {save_model_path}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-31T02:27:24.771174Z",
     "iopub.status.idle": "2025-10-31T02:27:24.771371Z",
     "shell.execute_reply.started": "2025-10-31T02:27:24.771277Z",
     "shell.execute_reply": "2025-10-31T02:27:24.771285Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}